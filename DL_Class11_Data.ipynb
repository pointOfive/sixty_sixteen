{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- TF Data API\n",
    "- TFRecord format\n",
    "- ~~standard Keras preprocessing layers~~\n",
    "- custom preprocessing layers\n",
    "- ~~TFDS~~\n",
    "- ~~tf.Transform~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "# a tensor\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What data will be in here? \n",
    "- What data frormat will this have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# here's some cool functionality\n",
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset = dataset.repeat(2).batch(4, drop_remainder=True).map(lambda x: x * 2, num_parallel_calls=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What will this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (4,), types: tf.int32>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 4 6], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14], shape=(4,), dtype=int32)\n",
      "tf.Tensor([16 18  0  2], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 4  6  8 10], shape=(4,), dtype=int32)\n",
      "tf.Tensor([12 14 16 18], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()\n",
    "# dataset = dataset.apply(tf.data.experimental.unbatch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What will this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10).batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What will this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 4 6 8], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0 2 4 6 8], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which of these will produce only the top line above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 4 6 8], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0 2 4 6 8], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# for item in dataset.take(1):\n",
    "for item in dataset.take(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([1 4 0], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
      "tf.Tensor([1 0 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([3 4 2], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5).repeat(3) # 0 to 4, three times\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(3) # `shuffle` has `seed` paramater, e.g., \"seed=42\"\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How is `suffle` above with `buffer_size=2` working?\n",
    "- (Hint #1: *batch is the last method applied*)\n",
    "- (Hint #2: *watch where 0 and 4 can land*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 3 1 4 2 8 7 5 9 6], shape=(10,), dtype=int64)\n",
      "tf.Tensor([0 3 1 4 2 8 7 5 9 6], shape=(10,), dtype=int64)\n",
      "tf.Tensor([0 3 1 4 2 8 7 5 9 6], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).shuffle(buffer_size=5, reshuffle_each_iteration=False)\n",
    "dataset = dataset.repeat(3).batch(10)\n",
    "for item in dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **WATCH OUT! Don't do the \"shuffle-then-repeat\" order above!**\n",
    "\n",
    "If you're not shuffling, the same order will be repeated at each epoch, and the model may end up being biased (e.g., due to some spurious patterns present by chance in the source dataâ€™s order).\n",
    "\n",
    "Here some things to think about!\n",
    "\n",
    "1. linux `shuf` can shuffle an original file\n",
    "2. if the original file is big\n",
    "    1. split the source data into multiple files\n",
    "    2. read them in a random order during training\n",
    "        - or even better: pick multiple files randomly and read them simultaneously, interleaving their records\n",
    "3. always have `tf.data.Dataset.shuffle` \"on\" \n",
    "\n",
    "This will help limit the potential for some weird \"order bias\" to affect things\n",
    "\n",
    "## Demo: here's how we can do this in just a few lines of code\n",
    "- **Going to use just use the pre-run from earlier today as the downloads were a bit flaky**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\n",
      "442368/441963 [==============================] - 3s 8us/step\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/datasets/_california_housing.py#L53\n",
    "# The original data can be found at:\n",
    "# https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file\n",
    "train_dataset_url = \"https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\"\n",
    "path = \"/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/\"\n",
    "file = \"calihousie.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=path+file, origin=train_dataset_url, untar=True, extract=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: the above only worked after a kernel restart and a few tries...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calihousie.csv.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/'\n",
    "! ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calihousie.csv.tar\r\n"
     ]
    }
   ],
   "source": [
    "! gunzip {path}calihousie.csv.tar.gz \n",
    "! ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/\r\n"
     ]
    }
   ],
   "source": [
    "! echo {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x CaliforniaHousing/cal_housing.data\r\n",
      "x CaliforniaHousing/cal_housing.domain\r\n"
     ]
    }
   ],
   "source": [
    "# https://askubuntu.com/questions/45349/how-to-extract-files-to-another-directory-using-tar-command\n",
    "! tar -xvf {path}calihousie.csv.tar -C {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCaliforniaHousing\u001b[m\u001b[m  calihousie.csv.tar\r\n"
     ]
    }
   ],
   "source": [
    "! ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20640 /Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l  {path}CaliforniaHousing/cal_housing.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below is just for functionality demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -4096 {path}CaliforniaHousing/cal_housing.data > tmp\n",
    "! mv tmp {path}CaliforniaHousing/cal_housing.data.test\n",
    "! head -16544 {path}CaliforniaHousing/cal_housing.data > tmp\n",
    "! mv tmp {path}CaliforniaHousing/cal_housing.data.train\n",
    "! tail -4096 {path}CaliforniaHousing/cal_housing.data.train > tmp\n",
    "! mv tmp {path}CaliforniaHousing/cal_housing.data.vali\n",
    "! head -12448 {path}CaliforniaHousing/cal_housing.data > tmp\n",
    "! mv tmp {path}CaliforniaHousing/cal_housing.data.train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test + rest\n",
    "4096+16544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16544"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rest + validation\n",
    "12448+4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test and validation can be split into 4 sets of 1024\n",
    "2**12/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we're going to pretend we had 6 \"very large\" separated data files, again for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_housing.data       cal_housing.data.4     cal_housing.data.train\r\n",
      "cal_housing.data.1     cal_housing.data.5     cal_housing.data.vali\r\n",
      "cal_housing.data.2     cal_housing.data.6     cal_housing.domain\r\n",
      "cal_housing.data.3     cal_housing.data.test\r\n"
     ]
    }
   ],
   "source": [
    "# https://blog.jpalardy.com/posts/how-to-shuffle-and-sample-on-the-command-line/\n",
    "# brew install coreutils\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.1\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.2\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.3\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.4\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.5\n",
    "! gshuf {path}CaliforniaHousing/cal_housing.data.train > {path}CaliforniaHousing/cal_housing.data.6\n",
    "! ls {path}CaliforniaHousing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19031144/how-to-split-one-text-file-into-multiple-txt-files\n",
    "! split -l 1024 {path}CaliforniaHousing/cal_housing.data.test {path}CaliforniaHousing/cal_housing.data.test.\n",
    "! split -l 1024 {path}CaliforniaHousing/cal_housing.data.vali {path}CaliforniaHousing/cal_housing.data.vali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_housing.data         cal_housing.data.test    cal_housing.data.vali.aa\r\n",
      "cal_housing.data.1       cal_housing.data.test.aa cal_housing.data.vali.ab\r\n",
      "cal_housing.data.2       cal_housing.data.test.ab cal_housing.data.vali.ac\r\n",
      "cal_housing.data.3       cal_housing.data.test.ac cal_housing.data.vali.ad\r\n",
      "cal_housing.data.4       cal_housing.data.test.ad cal_housing.domain\r\n",
      "cal_housing.data.5       cal_housing.data.train\r\n",
      "cal_housing.data.6       cal_housing.data.vali\r\n"
     ]
    }
   ],
   "source": [
    "! ls {path}CaliforniaHousing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAN THIS IN THE TERMINAL\n",
    "\n",
    "- It is too challenging to make complex bash work through a notebook\n",
    "- E.g., https://stackoverflow.com/questions/34972035/awk-print-with-pipes-not-working-ipython-in-jupyter-notebook\n",
    "- and the one-liners get *very* long: https://www.cyberciti.biz/faq/linux-unix-bash-for-loop-one-line-command/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO THIS WAS NOT RUN HERE: IT WAS RUN IN THE TERMINAL\n",
    "\n",
    "# https://stackoverflow.com/questions/9506810/add-column-to-end-of-csv-file-using-awk-in-bash-script\n",
    "# https://www.cyberciti.biz/faq/bash-for-loop/\n",
    "\n",
    "path=/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/\n",
    "for i in 1 2 3 4 5 6\n",
    "do \n",
    "   cat \"$path\"CaliforniaHousing/cal_housing.data.\"$i\" | awk -v i=$i -v OFS=, '{print i, $0}' > tmp\n",
    "   mv tmp \"$path\"CaliforniaHousing/cal_housing.data.\"$i\"\n",
    "done    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,-118.450000,34.210000,30.000000,2331.000000,733.000000,2172.000000,707.000000,2.188800,195600.000000\r\n",
      "1,-118.160000,34.600000,5.000000,7294.000000,1139.000000,3123.000000,930.000000,4.990400,154100.000000\r\n",
      "1,-118.370000,34.190000,41.000000,2924.000000,867.000000,2751.000000,836.000000,2.100000,171600.000000\r\n",
      "1,-117.820000,33.900000,25.000000,1137.000000,170.000000,524.000000,164.000000,7.574400,259300.000000\r\n",
      "1,-120.020000,39.240000,24.000000,1602.000000,426.000000,751.000000,257.000000,1.760900,99300.000000\r\n",
      "1,-118.050000,33.860000,16.000000,2676.000000,391.000000,1377.000000,395.000000,6.551300,350400.000000\r\n",
      "1,-121.920000,36.570000,42.000000,3944.000000,738.000000,1374.000000,598.000000,4.174000,394400.000000\r\n",
      "1,-118.150000,34.210000,34.000000,2765.000000,515.000000,1422.000000,438.000000,5.472700,238900.000000\r\n",
      "1,-117.800000,33.900000,22.000000,3760.000000,482.000000,1485.000000,461.000000,7.853700,354900.000000\r\n",
      "1,-115.600000,33.040000,31.000000,314.000000,61.000000,152.000000,56.000000,3.347200,91700.000000\r\n"
     ]
    }
   ],
   "source": [
    "! head {path}CaliforniaHousing/cal_housing.data.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,-118.140000,34.090000,20.000000,3447.000000,1007.000000,2622.000000,934.000000,2.918000,208700.000000\r\n",
      "2,-118.340000,33.830000,34.000000,1761.000000,329.000000,965.000000,329.000000,5.399000,358500.000000\r\n",
      "2,-122.030000,37.530000,18.000000,1746.000000,437.000000,1268.000000,404.000000,3.256000,183300.000000\r\n",
      "2,-121.280000,38.750000,52.000000,493.000000,89.000000,189.000000,94.000000,2.108000,83800.000000\r\n",
      "2,-119.640000,36.820000,14.000000,4872.000000,656.000000,2085.000000,617.000000,5.673900,173800.000000\r\n",
      "2,-118.150000,33.770000,39.000000,2428.000000,634.000000,1312.000000,612.000000,2.721200,266300.000000\r\n",
      "2,-119.450000,35.160000,34.000000,3437.000000,696.000000,1783.000000,608.000000,2.391200,52900.000000\r\n",
      "2,-118.220000,34.660000,17.000000,3810.000000,662.000000,1867.000000,586.000000,4.900000,152400.000000\r\n",
      "2,-118.090000,33.890000,42.000000,991.000000,215.000000,717.000000,219.000000,4.092600,164400.000000\r\n",
      "2,-118.470000,34.000000,38.000000,1235.000000,390.000000,891.000000,376.000000,2.714300,287500.000000\r\n"
     ]
    }
   ],
   "source": [
    "! head {path}CaliforniaHousing/cal_housing.data.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Starting up live coding again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.[0-9]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/'\n",
    "# watch out! this won't work!\n",
    "#train_filepaths = path+'CaliforniaHousing/cal_housing.data.*'\n",
    "train_filepaths = path+'CaliforniaHousing/cal_housing.data.[0-9]'\n",
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_filepaths = path+'CaliforniaHousing/cal_housing.data.vali.*'\n",
    "test_filepaths = path+'CaliforniaHousing/cal_housing.data.test.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.6', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.1', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths)#, seed=42)\n",
    "for item in filepath_dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.4', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.2', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in filepath_dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many more times can I run the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.3', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.6', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.1', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.4', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.5', shape=(), dtype=string)\n",
      "tf.Tensor(b'/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L11/CaliforniaHousing/cal_housing.data.2', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in filepath_dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 3\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath), # add `.skip(1)`, if header present\n",
    "    cycle_length=n_readers,\n",
    "    num_parallel_calls=1)#tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'6,-117.790000,33.690000,16.000000,3067.000000,396.000000,1275.000000,372.000000,8.738500,340000.000000'\n",
      "b'2,-118.140000,34.090000,20.000000,3447.000000,1007.000000,2622.000000,934.000000,2.918000,208700.000000'\n",
      "b'5,-117.750000,34.070000,52.000000,2550.000000,586.000000,1246.000000,576.000000,1.600600,146200.000000'\n",
      "b'6,-118.200000,33.970000,43.000000,825.000000,212.000000,820.000000,184.000000,1.889700,174300.000000'\n",
      "b'2,-118.340000,33.830000,34.000000,1761.000000,329.000000,965.000000,329.000000,5.399000,358500.000000'\n",
      "b'5,-117.350000,33.690000,11.000000,1229.000000,236.000000,581.000000,190.000000,3.102000,111300.000000'\n",
      "b'6,-119.120000,35.390000,13.000000,1264.000000,202.000000,552.000000,187.000000,4.590300,94300.000000'\n",
      "b'2,-122.030000,37.530000,18.000000,1746.000000,437.000000,1268.000000,404.000000,3.256000,183300.000000'\n",
      "b'5,-118.390000,34.190000,41.000000,2000.000000,485.000000,1439.000000,461.000000,3.049100,192000.000000'\n",
      "b'6,-118.310000,33.750000,36.000000,2715.000000,474.000000,1303.000000,457.000000,4.604200,357300.000000'\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "b'3,-118.180000,34.040000,42.000000,1670.000000,434.000000,1997.000000,452.000000,2.788000,150500.000000'\n",
      "b'1,-118.450000,34.210000,30.000000,2331.000000,733.000000,2172.000000,707.000000,2.188800,195600.000000'\n",
      "b'4,-118.150000,33.920000,28.000000,1038.000000,252.000000,912.000000,245.000000,2.587500,161200.000000'\n",
      "b'3,-118.190000,34.110000,40.000000,1266.000000,348.000000,1032.000000,315.000000,2.166700,150000.000000'\n",
      "b'1,-118.160000,34.600000,5.000000,7294.000000,1139.000000,3123.000000,930.000000,4.990400,154100.000000'\n",
      "b'4,-117.800000,33.690000,13.000000,1161.000000,289.000000,630.000000,296.000000,3.343800,333300.000000'\n",
      "b'3,-118.230000,33.920000,24.000000,1555.000000,406.000000,1665.000000,361.000000,1.643700,98800.000000'\n",
      "b'1,-118.370000,34.190000,41.000000,2924.000000,867.000000,2751.000000,836.000000,2.100000,171600.000000'\n",
      "b'4,-121.550000,39.440000,31.000000,1434.000000,283.000000,811.000000,289.000000,1.772700,49000.000000'\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "# taking enough to get through the first 3 files\n",
    "for item in dataset.take(3*12448+10):\n",
    "    \n",
    "    # printing out the first 10 and the next 10 after we get through the first three files\n",
    "    if i<10 or i>3*12448:\n",
    "        print(item.numpy())\n",
    "    if i==10:\n",
    "        print('.\\n.\\n.\\n')\n",
    "\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So what's the scheme here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
    "n_inputs = 8 # we've added the file indicator\n",
    "\n",
    "def preprocess(line):\n",
    "    \n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] # making it a tensor\n",
    "    fields = tf.io.decode_csv(line, select_cols=list(range(1,10)), record_defaults=defs) # list of tensor objects\n",
    "                                                                                         # sans file indicator column\n",
    "    x = tf.stack(fields[:-1]) # stack them into a feature vector (tensor datatype)\n",
    "    y = tf.stack(fields[-1:]) # create the corresponding output dimensions\n",
    "    #return (x - X_mean) / X_std, y\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-117.94  ,   34.15  ,   33.    ,  859.    ,  144.    ,  421.    ,\n",
       "         138.    ,    4.4821], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([220100.], dtype=float32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'1, -117.940000,34.150000,33.000000,859.000000,144.000000,421.000000,138.000000,4.482100,220100.000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "t = [0]\n",
    "for row in final_dataset.shuffle(1).repeat(1).batch(1):\n",
    "    t += row[0]\n",
    "    i += 1\n",
    "\n",
    "m = t/i\n",
    "s = [0]\n",
    "for row in final_dataset.shuffle(1).repeat(1).batch(1):\n",
    "    s += (row[0]-m)**2\n",
    "\n",
    "s = (s/(i-1))**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
       "array([[-119.18107  ,   35.261894 ,   29.80463  , 2566.0815   ,\n",
       "         532.49164  , 1424.7545   ,  493.855    ,    3.8675494]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
       "array([[1.7893246e+00, 1.9593276e+00, 1.2233307e+01, 2.2025779e+03,\n",
       "        4.2858249e+02, 1.1212059e+03, 3.8747769e+02, 1.9504192e+00]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = m,s # mean and scale of each feature in the training set\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, select_cols=list(range(1,10)), record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=2, n_read_threads=tf.data.experimental.AUTOTUNE,\n",
    "                       shuffle_buffer_size=10000, repeat=1, \n",
    "                       n_parse_threads=tf.data.experimental.AUTOTUNE,\n",
    "                       batch_size=32):\n",
    "    \n",
    "    # building the interleaved data set just as we did above\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath),\n",
    "                                 cycle_length=n_readers, \n",
    "                                 num_parallel_calls=n_read_threads)\n",
    "                                 # so that's the first row of arguments\n",
    "                                     \n",
    "    # now we have a rolling window, and we repeat that data set some number of duplications \n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    # `reshuffle_each_iteration=True` is the default in `shuffle`, so each repeat order will vary\n",
    "    \n",
    "    # and when the data comes out, we standarize it, and this may be threaded\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    \n",
    "    # finally we set the batch, and possibly get the next batch ready \n",
    "    # while the current batch is proocessing through the NN\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- If you have enough RAM, you might not want to re-preprocess at each epoch\n",
    "- There is a `.cache()` method for TF Datasets that avoids this\n",
    "- while still shuffling, repeating, batching, and prefetching distinctly at each epoch\n",
    "\n",
    "*Of course, you can always pre-preprocess...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other potentially useful data processing capabilities that might do just the trick for what you need in any given circumnstance:\n",
    "- `concatenate()`\n",
    "- `zip()`\n",
    "- `window()`\n",
    "- `reduce()`\n",
    "- `shard()`\n",
    "- `flat_map()` \n",
    "- `padded_batch()`\n",
    "- `from_generator()`\n",
    "- `from_tensors()`\n",
    "- `tf.data.experimental`\n",
    "    - `CsvDataset`\n",
    "    - `make_csv_dataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to skip this for now -- the basic idea should be clear\n",
    "- however because only the *train_filepaths* have the extra file indicator column we added, we would need to define the correct csv_reader_dataset2 to use on the *test_filepaths* and *vali_filepaths* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = csv_reader_dataset(train_filepaths)\n",
    "validation_dataset = csv_reader_dataset2(vali_filepaths)\n",
    "test_dataset = csv_reader_dataset2(test_filepaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4771/4771 [==============================] - 75s 16ms/step - loss: 205509.9688 - val_loss: 180268.1562\n",
      "Epoch 2/3\n",
      "4771/4771 [==============================] - 77s 16ms/step - loss: 198368.3281 - val_loss: 169853.0625\n",
      "Epoch 3/3\n",
      "4771/4771 [==============================] - 79s 17ms/step - loss: 184238.6562 - val_loss: 152307.3906\n"
     ]
    }
   ],
   "source": [
    "# instead of adding preprocessing in the map, you could also add it as a layer here\n",
    "# the \"Hands-On ML\" textbook walks through a couple ways to that:\n",
    "# - Lambda layer\n",
    "# - Custom layer\n",
    "# - `tf.keras.layers.Normalization`\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.Input(shape=(1,8,)),\n",
    "  tf.keras.layers.Dense(1024),\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.ReLU(),\n",
    "  tf.keras.layers.Dense(512),\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.ReLU(),\n",
    "  tf.keras.layers.Dense(256),\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.ReLU(),\n",
    "  tf.keras.layers.Dense(128),\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.ReLU(),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=tf.keras.losses.MAE)\n",
    "# https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy\n",
    "history = model.fit(train_dataset, epochs=3, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f90ee5e1820>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#history.history.keys()\n",
    "plt.plot(history.history['loss'],label='loss')\n",
    "plt.plot(history.history['val_loss'],label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4771/4771 [==============================] - 6s 1ms/step - loss: 174910.2344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174910.234375"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate(train_dataset)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36999.676, 36773.45 , 37088.066], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_dataset.take(1).map(lambda X, y: X) # take returns a batch\n",
    "model.predict(new_set)[:3,0,0] # a dataset containing new instances\n",
    "# just looking at first 3 of the new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASS 13\n",
    "\n",
    "- Continuing through Hands-On ML Ch 13 TFDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord\n",
    "- Efficient Data Binarization Optimized for TF\n",
    "- Some nice [explanation](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564) and further [documentation](https://www.tensorflow.org/tutorials/load_data/tfrecord) are easy to find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000ï¿½K\"This is the first recordï¿½U^\u001d",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000`/ï¿½#And this is the second recordï¿½ï¿½ï¿½0"
     ]
    }
   ],
   "source": [
    "#! rm my_compressed.tfrecord my_data.tfrecord calihousie.tfrecord\n",
    "#! ls\n",
    "! head my_data.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001fï¿½\b\u0000\u0000\u0000\u0000\u0000\u0000\u0013ï¿½`ï¿½ï¿½ï¿½ï¿½ï¿½J!\u0019ï¿½ï¿½\r\n",
      "@Tï¿½ï¿½ï¿½ï¿½ï¿½YT\\ï¿½Pï¿½ï¿½ï¿½_ï¿½ï¿½r{hï¿½,T]ï¿½ï¿½}eÇ¼\u0014ï¿½\u001aï¿½ï¿½bï¿½*ï¿½\u0010Dï¿½\u001b\u0006\u0000?,hï¿½U\u0000\u0000\u0000"
     ]
    }
   ],
   "source": [
    "! head my_compressed.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      79 my_compressed.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "! wc -c my_compressed.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      85 my_data.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "! wc -c my_data.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"])\n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                   compression_type=\"GZIP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'This is the first record'\n",
      "b'And this is the second record'\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So there is some redundancy and understanding of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's return to the previous data set we were looking at\n",
    "- We'll package it up in a TFRecord form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.5371117 -0.5776947  1.8143393 -0.8059109 -0.8364589 -0.8238937\n",
      "  -0.8564493 -0.2631996]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = csv_reader_dataset(train_filepaths)\n",
    "\n",
    "for item in train_dataset.take(1):\n",
    "    print(item[0][0,...])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's what it looks like make a TFRecord from the first row of data\n",
    "- Highly structured and formatted representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"households\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.8564493060112\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"housingMedianAge\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 1.814339280128479\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"latitude\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.5776947140693665\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"longitude\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 0.5371116995811462\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"medianHouseValue\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 178100.0\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"medianIncome\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.2631995975971222\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"population\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.8238937258720398\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"totalBedrooms\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.8364589214324951\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"totalRooms\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: -0.8059108853340149\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calihouse_example = Example(\n",
    "  features=Features(\n",
    "    feature={\n",
    "    \"longitude\":        Feature(float_list=FloatList(value=[item[0][0,0,0].numpy()])),\n",
    "    \"latitude\":         Feature(float_list=FloatList(value=[item[0][0,0,1].numpy()])),\n",
    "    \"housingMedianAge\": Feature(float_list=FloatList(value=[item[0][0,0,2].numpy()])),\n",
    "    \"totalRooms\":       Feature(float_list=FloatList(value=[item[0][0,0,3].numpy()])),\n",
    "    \"totalBedrooms\":    Feature(float_list=FloatList(value=[item[0][0,0,4].numpy()])),\n",
    "    \"population\":       Feature(float_list=FloatList(value=[item[0][0,0,5].numpy()])),\n",
    "    \"households\":       Feature(float_list=FloatList(value=[item[0][0,0,6].numpy()])),\n",
    "    \"medianIncome\":     Feature(float_list=FloatList(value=[item[0][0,0,7].numpy()])),\n",
    "    \"medianHouseValue\": Feature(float_list=FloatList(value=[item[1][0].numpy()]))\n",
    "# Example from textbook looked something like this:    \n",
    "#   \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "#   \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "#   \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "\n",
    "}))\n",
    "\n",
    "calihouse_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can write it out to a file as a `TFRecord` with `SerializeToString()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xe6\\x01\\n\\x16\\n\\npopulation\\x12\\x08\\x12\\x06\\n\\x04\\xb3\\xeaR\\xbf\\n\\x19\\n\\rtotalBedrooms\\x12\\x08\\x12\\x06\\n\\x04,\"V\\xbf\\n\\x14\\n\\x08latitude\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xe3\\x13\\xbf\\n\\x16\\n\\nhouseholds\\x12\\x08\\x12\\x06\\n\\x04C@[\\xbf\\n\\x1c\\n\\x10housingMedianAge\\x12\\x08\\x12\\x06\\n\\x04E<\\xe8?\\n\\x16\\n\\ntotalRooms\\x12\\x08\\x12\\x06\\n\\x04-PN\\xbf\\n\\x15\\n\\tlongitude\\x12\\x08\\x12\\x06\\n\\x04\\'\\x80\\t?\\n\\x18\\n\\x0cmedianIncome\\x12\\x08\\x12\\x06\\n\\x04\\x19\\xc2\\x86\\xbe\\n\\x1c\\n\\x10medianHouseValue\\x12\\x08\\x12\\x06\\n\\x04\\x00\\xed-H'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calihouse_example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "with tf.io.TFRecordWriter(\"calihousie.tfrecord\") as f:\n",
    "\n",
    "    f.write(calihouse_example.SerializeToString())\n",
    "    # writing it twice, just for demo purposes    \n",
    "    f.write(calihouse_example.SerializeToString())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\u0000\u0000\u0000\u0000\u0000\u0000\u000053ï¿½\r\n",
      "ï¿½\u0001\r\n",
      "\u0016\r\n",
      "\r\n",
      "population\u0012\b\u0012\u0006\r\n",
      "\u0004ï¿½ï¿½Rï¿½\r\n",
      "\u0019\r\n",
      "\r",
      "totalBedrooms\u0012\b\u0012\u0006\r\n",
      "\u0004,\"Vï¿½\r\n",
      "\u0014\r\n"
     ]
    }
   ],
   "source": [
    "! head calihousie.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     498 calihousie.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "! wc -c calihousie.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading it back in just returns the serialized string...\n",
    "- which we can then parse back into a TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\xe6\\x01\\n\\x16\\n\\npopulation\\x12\\x08\\x12\\x06\\n\\x04\\xb3\\xeaR\\xbf\\n\\x19\\n\\rtotalBedrooms\\x12\\x08\\x12\\x06\\n\\x04,\"V\\xbf\\n\\x14\\n\\x08latitude\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xe3\\x13\\xbf\\n\\x16\\n\\nhouseholds\\x12\\x08\\x12\\x06\\n\\x04C@[\\xbf\\n\\x1c\\n\\x10housingMedianAge\\x12\\x08\\x12\\x06\\n\\x04E<\\xe8?\\n\\x16\\n\\ntotalRooms\\x12\\x08\\x12\\x06\\n\\x04-PN\\xbf\\n\\x15\\n\\tlongitude\\x12\\x08\\x12\\x06\\n\\x04\\'\\x80\\t?\\n\\x18\\n\\x0cmedianIncome\\x12\\x08\\x12\\x06\\n\\x04\\x19\\xc2\\x86\\xbe\\n\\x1c\\n\\x10medianHouseValue\\x12\\x08\\x12\\x06\\n\\x04\\x00\\xed-H', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'\\n\\xe6\\x01\\n\\x16\\n\\npopulation\\x12\\x08\\x12\\x06\\n\\x04\\xb3\\xeaR\\xbf\\n\\x19\\n\\rtotalBedrooms\\x12\\x08\\x12\\x06\\n\\x04,\"V\\xbf\\n\\x14\\n\\x08latitude\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xe3\\x13\\xbf\\n\\x16\\n\\nhouseholds\\x12\\x08\\x12\\x06\\n\\x04C@[\\xbf\\n\\x1c\\n\\x10housingMedianAge\\x12\\x08\\x12\\x06\\n\\x04E<\\xe8?\\n\\x16\\n\\ntotalRooms\\x12\\x08\\x12\\x06\\n\\x04-PN\\xbf\\n\\x15\\n\\tlongitude\\x12\\x08\\x12\\x06\\n\\x04\\'\\x80\\t?\\n\\x18\\n\\x0cmedianIncome\\x12\\x08\\x12\\x06\\n\\x04\\x19\\xc2\\x86\\xbe\\n\\x1c\\n\\x10medianHouseValue\\x12\\x08\\x12\\x06\\n\\x04\\x00\\xed-H', shape=(), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"calihousie.tfrecord\"])\n",
    "for item in dataset.take(2):\n",
    "    print(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "features {\n",
      "  feature {\n",
      "    key: \"households\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8564493060112\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"housingMedianAge\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 1.814339280128479\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.5776947140693665\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.5371116995811462\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"medianHouseValue\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 178100.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"medianIncome\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.2631995975971222\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"population\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8238937258720398\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"totalBedrooms\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8364589214324951\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"totalRooms\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8059108853340149\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "1\n",
      "features {\n",
      "  feature {\n",
      "    key: \"households\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8564493060112\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"housingMedianAge\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 1.814339280128479\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"latitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.5776947140693665\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"longitude\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.5371116995811462\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"medianHouseValue\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 178100.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"medianIncome\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.2631995975971222\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"population\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8238937258720398\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"totalBedrooms\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8364589214324951\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"totalRooms\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.8059108853340149\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = Example()\n",
    "for i,item in enumerate(dataset.take(2)):\n",
    "    print(i)\n",
    "    example.ParseFromString(item.numpy())\n",
    "    print(example)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can get TF Tensor form with `tf.io.parse_single_example()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'medianHouseValue': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc7aa61dbe0>,\n",
       " 'households': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8564493>,\n",
       " 'housingMedianAge': <tf.Tensor: shape=(), dtype=float32, numpy=1.8143393>,\n",
       " 'latitude': <tf.Tensor: shape=(), dtype=float32, numpy=-0.5776947>,\n",
       " 'longitude': <tf.Tensor: shape=(), dtype=float32, numpy=0.5371117>,\n",
       " 'medianIncome': <tf.Tensor: shape=(), dtype=float32, numpy=-0.2631996>,\n",
       " 'population': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8238937>,\n",
       " 'totalBedrooms': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8364589>,\n",
       " 'totalRooms': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8059109>}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_description = {\n",
    "            \"longitude\":        tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"latitude\":         tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"housingMedianAge\": tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"totalRooms\":       tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"totalBedrooms\":    tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"population\":       tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"households\":       tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"medianIncome\":     tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "            \"medianHouseValue\": tf.io.VarLenFeature(tf.float32)\n",
    "}\n",
    "# https://stackoverflow.com/questions/41921746/tensorflow-varlenfeature-vs-fixedlenfeature\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"calihousie.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                feature_description)\n",
    "    \n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a `sparse` tensor?\n",
    "- `FixedLenFeature` vs `VarLenFeature`\n",
    "- Some data will be variable, and not just like different sized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.8564493>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc7aa61dbe0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['medianHouseValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([178100.], dtype=float32)>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['medianHouseValue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([178100.], dtype=float32)>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"medianHouseValue\"], default_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can get batches with `tf.io.parse_example()`\n",
    "- Further examples in context can be found in the [TF Documentation]( https://www.tensorflow.org/api_docs/python/tf/io/parse_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'medianHouseValue': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc7aa64b3d0>,\n",
       " 'households': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8564493, -0.8564493], dtype=float32)>,\n",
       " 'housingMedianAge': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.8143393, 1.8143393], dtype=float32)>,\n",
       " 'latitude': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.5776947, -0.5776947], dtype=float32)>,\n",
       " 'longitude': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5371117, 0.5371117], dtype=float32)>,\n",
       " 'medianIncome': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.2631996, -0.2631996], dtype=float32)>,\n",
       " 'population': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8238937, -0.8238937], dtype=float32)>,\n",
       " 'totalBedrooms': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8364589, -0.8364589], dtype=float32)>,\n",
       " 'totalRooms': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8059109, -0.8059109], dtype=float32)>}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for serialized_examples in tf.data.TFRecordDataset([\"calihousie.tfrecord\"]).batch(2):\n",
    "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
    "                                          feature_description)\n",
    "parsed_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data Types\n",
    "- Notebooks for GÃ©ron's \"Hands-on ML\" book are available [on github](https://github.com/ageron/handson-ml2)\n",
    "- Examples of the data types below can be found in the [Chapter 13 notebook](https://github.com/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb)\n",
    "\n",
    "### Data storage for images:\n",
    "- `Feature(bytes_list=BytesList(value=tf.io.encode_jpeg()))`\n",
    "    - `tf.io.decode_jpeg()`\n",
    "    - `tf.io.decode_image()`\n",
    "\n",
    "### Data storage for sequences; i.e., \"lists of lists\" stuff:\n",
    "- `SequenceExample`\n",
    "    - `tf.io.parse_single_sequence_example()`\n",
    "    - `tf.io.parse_sequence_example()`\n",
    "    - `tf.RaggedTensor.from_sparse()`\n",
    "\n",
    "*Some code lifted from the notebook provided with the chapter:*\n",
    "\n",
    "```\n",
    "parsed_context, parsed_feature_lists = \n",
    "tf.io.parse_single_sequence_example(serialized_sequence_example,\n",
    "                                    context_feature_descriptions, \n",
    "                                    sequence_feature_descriptions)\n",
    "                                                                           \n",
    "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
    "```\n",
    "\n",
    "### And you can even store tensors directly as themselves\n",
    "\n",
    "- `Feature(bytes_list=BytesList(value=tf.io.serialize_tensor()))`?\n",
    "    - `tf.io.parse_tensor()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Examples of these found in GÃ©ron's \"Hands-on ML\" [Chapter 13 notebook](https://github.com/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places where you can get some data\n",
    "\n",
    "- [TFDS](https://homl.info/tfds)\n",
    "- https://learning.oreilly.com/library/view/Hands-On+Machine+Learning+with+Scikit-Learn,+Keras,+and+TensorFlow,+2nd+Edition/9781492032632/ch02.html#project_chapter\n",
    "- [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    "- [Amazonâ€™s AWS datasets](https://registry.opendata.aws/)\n",
    "- [Google Data Search](https://datasetsearch.research.google.com/)\n",
    "- [Kaggle datasets](https://www.kaggle.com/datasets)\n",
    "- [data.world](https://data.world/search)\n",
    "- [Data Portals](http://dataportals.org/)\n",
    "- [OpenDataMonitor](http://opendatamonitor.eu/)\n",
    "- [Quandl](http://quandl.com/)\n",
    "- [Wikipediaâ€™s list of Machine Learning datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)\n",
    "- [Quora.com](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "- [The datasets subreddit](https://www.reddit.com/r/datasets)\n",
    "- [tycho](https://www.tycho.pitt.edu/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "                                                                               failed\n",
      "\n",
      "UnsatisfiableError: The following specifications were found to be incompatible with each other:\n",
      "\n",
      "Output in format: Requested package -> Available versions\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# https://anaconda.org/anaconda/tensorflow-datasets\n",
    "! yes y | conda install -c anaconda tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.6 MB 503 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (20.2.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: numpy in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (1.18.5)\n",
      "Requirement already satisfied: termcolor in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (0.10.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70 kB 671 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829 kB 634 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81 kB 667 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44 kB 735 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (49.6.0.post20200925)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100 kB 674 kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: promise, future\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=16e450f0a7fda57ac402ea2ae3b5b07286d8567d30b6c7df01b0432579fdba59\n",
      "  Stored in directory: /Users/gck8gd/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=d5fa90510e04a58d8d8f7c6091ac9df93c90c4bef3766386292f801c97418089\n",
      "  Stored in directory: /Users/gck8gd/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built promise future\n",
      "Installing collected packages: promise, tqdm, future, dill, googleapis-common-protos, tensorflow-metadata, importlib-resources, tensorflow-datasets\n",
      "Successfully installed dill-0.3.3 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 tqdm-4.51.0\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/overview\n",
    "! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.5\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.11.8  |       h033912b_0         145 KB  conda-forge\n",
      "    certifi-2020.11.8          |   py38h50d1736_0         150 KB  conda-forge\n",
      "    ipywidgets-7.5.1           |     pyh9f0ad1d_1         101 KB  conda-forge\n",
      "    openssl-1.1.1h             |       haf1e3a3_0         1.9 MB  conda-forge\n",
      "    widgetsnbextension-3.5.1   |   py38h32f6830_4         1.8 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-7.5.1-pyh9f0ad1d_1\n",
      "  widgetsnbextension conda-forge/osx-64::widgetsnbextension-3.5.1-py38h32f6830_4\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2020.10.14~ --> conda-forge::ca-certificates-2020.11.8-h033912b_0\n",
      "  certifi            pkgs/main/noarch::certifi-2020.6.20-p~ --> conda-forge/osx-64::certifi-2020.11.8-py38h50d1736_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  openssl                                         pkgs/main --> conda-forge\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1h       | 1.9 MB    | ##################################### | 100% \n",
      "widgetsnbextension-3 | 1.8 MB    | ##################################### | 100% \n",
      "certifi-2020.11.8    | 150 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 145 KB    | ##################################### | 100% \n",
      "ipywidgets-7.5.1     | 101 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "done\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "! yes y | conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/57343134/jupyter-notebooks-not-displaying-progress-bars\n",
    "# ` brew install node`\n",
    "! jupyter nbextension enable --py widgetsnbextension\n",
    "# did not have to complete this:\n",
    "#! yes y | jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# but note that I did need to follow this direction:\n",
    "# 'It should work now after refreshing the Jupyter browser tab.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset bool_q/1.0.0 (download: 8.36 MiB, generated: 8.51 MiB, total: 16.87 MiB) to /Users/gck8gd/tensorflow_datasets/bool_q/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29093c9e811423a985261e571651143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b3bbccabc0451b81d351f0184220fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/gck8gd/tensorflow_datasets/bool_q/1.0.0.incompleteQIIJ6W/bool_q-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb4ed5dc579484290ac70b093b9a1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/gck8gd/tensorflow_datasets/bool_q/1.0.0.incompleteQIIJ6W/bool_q-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c910312eb3241bf86105e4fad5d19df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset bool_q downloaded and prepared to /Users/gck8gd/tensorflow_datasets/bool_q/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#! rm -r /Users/gck8gd/tensorflow_datasets/bool_q\n",
    "dataset = tfds.load(name=\"bool_q\", shuffle_files=True)#, batch_size=32)\n",
    "\n",
    "# https://www.tensorflow.org/datasets/catalog/bool_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "as_supervised=True but bool_q does not support a supervised (input, label) structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-4393044a3fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bool_q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"read_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_supervised\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_single_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0;31m# Could add support for more exotic data_struct, like OrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     return {\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     }\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, shuffle_files, batch_size, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervised_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0;34m\"as_supervised=True but %s does not support a supervised \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \"(input, label) structure.\" % self.name)\n",
      "\u001b[0;31mValueError\u001b[0m: as_supervised=True but bool_q does not support a supervised (input, label) structure."
     ]
    }
   ],
   "source": [
    "dataset = tfds.load(name=\"bool_q\", shuffle_files=True, batch_size=32, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': <tf.Tensor: shape=(), dtype=bool, numpy=True>,\n",
       " 'passage': <tf.Tensor: shape=(), dtype=string, numpy=b'Death from laughter is a rare form of death, usually resulting from cardiac arrest or asphyxiation, caused by a fit of laughter. Instances of death by laughter have been recorded from the times of ancient Greece to the modern day.'>,\n",
       " 'question': <tf.Tensor: shape=(), dtype=string, numpy=b'is it possible for someone to die of laughter'>,\n",
       " 'title': <tf.Tensor: shape=(), dtype=string, numpy=b'Death from laughter'>}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['validation'].take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer', 'passage', 'question', 'title'])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['validation'].take(1))).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"This version of the fairy tale character has been very well received by film critics and the public, and is considered one of Disney's most iconic and menacing villains. Besides in the film, the Evil Queen has made numerous appearances in Disney attractions and productions, including not only these directly related to the tale of Snow White, such as Fantasmic!, The Kingdom Keepers and Kingdom Hearts Birth by Sleep, sometimes appearing in them alongside Maleficent from Sleeping Beauty. The film's version of the Queen has also become a popular archetype that influenced a number of artists and non-Disney works.\">"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['validation'].take(1)))['passage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we make this data?\n",
    "- Text Vectorization and Embeddings (i.e., Dense Representations)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization\n",
    "#https://towardsdatascience.com/you-should-try-the-new-tensorflows-textvectorization-layer-a80b3c6b00ee \n",
    "tf.keras.layers.experimental.preprocessing.TextVectorization?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextVectorization = tf.keras.layers.experimental.preprocessing.TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=500,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/50737192/tf-data-dataset-how-to-get-the-dataset-size-number-of-elements-in-a-epoch\n",
    "tf.data.experimental.cardinality(dataset['validation']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_MODULE_IGNORED_PROPERTIES',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__metaclass__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activity_regularizer',\n",
       " '_add_state_variable',\n",
       " '_add_trackable',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_assert_same_type',\n",
       " '_auto_track_sub_layers',\n",
       " '_autocast',\n",
       " '_autographed_call',\n",
       " '_build_input_shape',\n",
       " '_call_accepts_kwargs',\n",
       " '_call_arg_was_passed',\n",
       " '_call_fn_arg_defaults',\n",
       " '_call_fn_arg_positions',\n",
       " '_call_fn_args',\n",
       " '_call_full_argspec',\n",
       " '_callable_losses',\n",
       " '_called',\n",
       " '_cast_single_input',\n",
       " '_checkpoint_dependencies',\n",
       " '_clear_losses',\n",
       " '_combiner',\n",
       " '_compute_dtype',\n",
       " '_compute_dtype_object',\n",
       " '_convert_to_ndarray',\n",
       " '_dedup_weights',\n",
       " '_default_training_arg',\n",
       " '_deferred_dependencies',\n",
       " '_dtype',\n",
       " '_dtype_defaulted_to_floatx',\n",
       " '_dtype_policy',\n",
       " '_dynamic',\n",
       " '_eager_losses',\n",
       " '_expects_mask_arg',\n",
       " '_expects_training_arg',\n",
       " '_flatten',\n",
       " '_flatten_layers',\n",
       " '_functional_construction_call',\n",
       " '_gather_children_attribute',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_get_call_arg_value',\n",
       " '_get_dataset_iterator',\n",
       " '_get_existing_metric',\n",
       " '_get_index_lookup_class',\n",
       " '_get_input_masks',\n",
       " '_get_node_attribute_at_index',\n",
       " '_get_save_spec',\n",
       " '_get_trainable_state',\n",
       " '_get_vectorization_class',\n",
       " '_handle_activity_regularization',\n",
       " '_handle_deferred_dependencies',\n",
       " '_handle_weight_regularization',\n",
       " '_inbound_nodes',\n",
       " '_index_lookup_layer',\n",
       " '_infer_output_signature',\n",
       " '_init_call_fn_args',\n",
       " '_init_set_name',\n",
       " '_initial_weights',\n",
       " '_input_spec',\n",
       " '_is_layer',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_keras_tensor_symbolic_call',\n",
       " '_layers',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_losses',\n",
       " '_map_resources',\n",
       " '_max_tokens',\n",
       " '_maybe_build',\n",
       " '_maybe_cast_inputs',\n",
       " '_maybe_create_attribute',\n",
       " '_maybe_initialize_trackable',\n",
       " '_metrics',\n",
       " '_metrics_lock',\n",
       " '_must_restore_from_config',\n",
       " '_name',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_name_scope',\n",
       " '_ngrams',\n",
       " '_ngrams_arg',\n",
       " '_no_dependency',\n",
       " '_non_trainable_weights',\n",
       " '_obj_reference_counts',\n",
       " '_obj_reference_counts_dict',\n",
       " '_object_identifier',\n",
       " '_oov_value',\n",
       " '_outbound_nodes',\n",
       " '_output_mode',\n",
       " '_output_sequence_length',\n",
       " '_pad_to_max',\n",
       " '_preload_simple_restoration',\n",
       " '_preprocess',\n",
       " '_previously_updated',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_restore_updates',\n",
       " '_saved_model_inputs_spec',\n",
       " '_self_name_based_restores',\n",
       " '_self_saveable_object_factories',\n",
       " '_self_setattr_tracking',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_update_uid',\n",
       " '_set_call_arg_value',\n",
       " '_set_connectivity_metadata',\n",
       " '_set_dtype_policy',\n",
       " '_set_mask_keras_history_checked',\n",
       " '_set_mask_metadata',\n",
       " '_set_save_spec',\n",
       " '_set_state_variables',\n",
       " '_set_trainable_state',\n",
       " '_set_training_mode',\n",
       " '_setattr_tracking',\n",
       " '_should_cast_single_input',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_split',\n",
       " '_split_out_first_arg',\n",
       " '_standardize',\n",
       " '_stateful',\n",
       " '_supports_masking',\n",
       " '_symbolic_call',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_thread_local',\n",
       " '_track_trackable',\n",
       " '_trackable_saved_model_saver',\n",
       " '_tracking_metadata',\n",
       " '_trainable',\n",
       " '_trainable_weights',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " '_updates',\n",
       " '_vectorize_layer',\n",
       " '_vocab_size',\n",
       " '_warn_about_input_casting',\n",
       " 'activity_regularizer',\n",
       " 'adapt',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'add_variable',\n",
       " 'add_weight',\n",
       " 'apply',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'compute_output_signature',\n",
       " 'count_params',\n",
       " 'dtype',\n",
       " 'dynamic',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_vocabulary',\n",
       " 'get_weights',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'non_trainable_weights',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_shape',\n",
       " 'set_vocabulary',\n",
       " 'set_weights',\n",
       " 'state_variables',\n",
       " 'stateful',\n",
       " 'submodules',\n",
       " 'supports_masking',\n",
       " 'trainable',\n",
       " 'trainable_variables',\n",
       " 'trainable_weights',\n",
       " 'updates',\n",
       " 'variables',\n",
       " 'weights',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'of',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'to',\n",
       " 'is',\n",
       " 'as',\n",
       " 'by',\n",
       " 'on',\n",
       " 'for',\n",
       " 'with',\n",
       " 'or',\n",
       " 'was',\n",
       " 'it',\n",
       " 'that',\n",
       " 'are',\n",
       " 'from',\n",
       " 'an',\n",
       " 'be',\n",
       " 'at',\n",
       " 'which',\n",
       " 'not',\n",
       " 'has',\n",
       " 'also',\n",
       " 'have',\n",
       " 'states',\n",
       " 'its',\n",
       " 'their',\n",
       " 'united',\n",
       " 'series',\n",
       " 'this',\n",
       " 'one',\n",
       " 'first',\n",
       " 'they',\n",
       " 'but',\n",
       " 'other',\n",
       " 'film',\n",
       " 'may',\n",
       " 'new',\n",
       " 'two',\n",
       " 'his',\n",
       " 'season',\n",
       " 'after',\n",
       " 'can',\n",
       " 'world',\n",
       " 'he',\n",
       " 'been',\n",
       " 'used',\n",
       " 'all',\n",
       " 'her',\n",
       " 'most',\n",
       " 'who',\n",
       " 'when',\n",
       " 'were',\n",
       " 'only',\n",
       " 'into',\n",
       " 'such',\n",
       " 'american',\n",
       " 'time',\n",
       " 'us',\n",
       " 'than',\n",
       " 'more',\n",
       " 'state',\n",
       " 'known',\n",
       " 'some',\n",
       " 'between',\n",
       " 'if',\n",
       " 'she',\n",
       " '2018',\n",
       " 'while',\n",
       " 'had',\n",
       " '2017',\n",
       " 'there',\n",
       " 'no',\n",
       " 'since',\n",
       " 'however',\n",
       " 'being',\n",
       " 'game',\n",
       " 'years',\n",
       " 'during',\n",
       " 'cup',\n",
       " 'any',\n",
       " 'over',\n",
       " 'both',\n",
       " 'same',\n",
       " 'many',\n",
       " 'three',\n",
       " 'under',\n",
       " 'second',\n",
       " 'called',\n",
       " 'about',\n",
       " 'will',\n",
       " 'law',\n",
       " 'where',\n",
       " 'team',\n",
       " 'through',\n",
       " 'up',\n",
       " 'including',\n",
       " 'games',\n",
       " 'would',\n",
       " 'each',\n",
       " 'city',\n",
       " 'use',\n",
       " 'these',\n",
       " 'often',\n",
       " 'national',\n",
       " 'number',\n",
       " 'name',\n",
       " 'before',\n",
       " 'made',\n",
       " 'system',\n",
       " 'released',\n",
       " 'part',\n",
       " 'well',\n",
       " 'until',\n",
       " 'third',\n",
       " 'then',\n",
       " 'four',\n",
       " 'north',\n",
       " 'area',\n",
       " 'based',\n",
       " 'later',\n",
       " 'although',\n",
       " 'out',\n",
       " '2015',\n",
       " 'set',\n",
       " 'usually',\n",
       " 'south',\n",
       " 'water',\n",
       " 'television',\n",
       " 'because',\n",
       " 'within',\n",
       " '2',\n",
       " 'family',\n",
       " 'group',\n",
       " 'york',\n",
       " 'him',\n",
       " '10',\n",
       " 'company',\n",
       " 'them',\n",
       " '2016',\n",
       " 'show',\n",
       " 'year',\n",
       " 'does',\n",
       " '1',\n",
       " 'federal',\n",
       " 'due',\n",
       " 'war',\n",
       " 'player',\n",
       " 'must',\n",
       " 'last',\n",
       " 'day',\n",
       " 'high',\n",
       " 'common',\n",
       " 'written',\n",
       " 'won',\n",
       " 'june',\n",
       " 'people',\n",
       " 'original',\n",
       " 'end',\n",
       " 'ball',\n",
       " 'final',\n",
       " 'following',\n",
       " 'different',\n",
       " 'canada',\n",
       " '2014',\n",
       " 'league',\n",
       " 'found',\n",
       " 'sometimes',\n",
       " 'september',\n",
       " 'april',\n",
       " 'major',\n",
       " 'home',\n",
       " 'several',\n",
       " 'red',\n",
       " 'five',\n",
       " 'countries',\n",
       " 'around',\n",
       " 'those',\n",
       " 'public',\n",
       " 'include',\n",
       " 'place',\n",
       " 'march',\n",
       " 'xbox',\n",
       " 'body',\n",
       " 'so',\n",
       " 'fifa',\n",
       " 'october',\n",
       " 'do',\n",
       " 'best',\n",
       " 'played',\n",
       " 'form',\n",
       " 'million',\n",
       " 'right',\n",
       " 'commonly',\n",
       " 'america',\n",
       " 'air',\n",
       " 'produced',\n",
       " 'government',\n",
       " 'british',\n",
       " 'act',\n",
       " 'premiered',\n",
       " 'football',\n",
       " 'even',\n",
       " 'directed',\n",
       " 'became',\n",
       " '3',\n",
       " 'large',\n",
       " 'island',\n",
       " 'episodes',\n",
       " 'book',\n",
       " 'announced',\n",
       " 'age',\n",
       " 'still',\n",
       " 'players',\n",
       " 'kingdom',\n",
       " '2010',\n",
       " 'service',\n",
       " 'i',\n",
       " 'white',\n",
       " 'typically',\n",
       " 'located',\n",
       " 'july',\n",
       " '4',\n",
       " 'times',\n",
       " 'films',\n",
       " 'referred',\n",
       " 'own',\n",
       " 'another',\n",
       " 'small',\n",
       " 'english',\n",
       " 'without',\n",
       " 'though',\n",
       " 'species',\n",
       " 'person',\n",
       " 'like',\n",
       " 'life',\n",
       " 'story',\n",
       " 'main',\n",
       " 'largest',\n",
       " 'having',\n",
       " 'term',\n",
       " 'song',\n",
       " 'member',\n",
       " 'either',\n",
       " 'similar',\n",
       " 'per',\n",
       " 'legal',\n",
       " 'house',\n",
       " 'created',\n",
       " 'against',\n",
       " 'six',\n",
       " 'park',\n",
       " 'california',\n",
       " '2013',\n",
       " 'school',\n",
       " 'novel',\n",
       " 'light',\n",
       " 'january',\n",
       " 'court',\n",
       " 'members',\n",
       " 'long',\n",
       " '2011',\n",
       " '15',\n",
       " 'generally',\n",
       " 'total',\n",
       " 'black',\n",
       " 'back',\n",
       " 'central',\n",
       " 'along',\n",
       " 'teams',\n",
       " 'single',\n",
       " 'power',\n",
       " 'days',\n",
       " 'international',\n",
       " 'fourth',\n",
       " 'range',\n",
       " 'order',\n",
       " 'months',\n",
       " 'february',\n",
       " 'production',\n",
       " 'off',\n",
       " 'left',\n",
       " 'laws',\n",
       " 'example',\n",
       " 'began',\n",
       " '21',\n",
       " '12',\n",
       " 'west',\n",
       " 'result',\n",
       " 'renewed',\n",
       " 'period',\n",
       " 'now',\n",
       " 'force',\n",
       " 'considered',\n",
       " '8',\n",
       " 'university',\n",
       " 'november',\n",
       " 'make',\n",
       " 'm',\n",
       " '11',\n",
       " 'required',\n",
       " 'president',\n",
       " 'character',\n",
       " '18',\n",
       " 'using',\n",
       " 'published',\n",
       " 'play',\n",
       " 'line',\n",
       " 'august',\n",
       " 'received',\n",
       " 'less',\n",
       " 'early',\n",
       " 'december',\n",
       " 'carry',\n",
       " 'base',\n",
       " 'drama',\n",
       " '30',\n",
       " '2012',\n",
       " 'stars',\n",
       " 'european',\n",
       " 'title',\n",
       " 'sequel',\n",
       " 'death',\n",
       " '5',\n",
       " 'local',\n",
       " 'least',\n",
       " 'much',\n",
       " 'constitution',\n",
       " 'card',\n",
       " 'very',\n",
       " 'bank',\n",
       " 'km',\n",
       " 'episode',\n",
       " 'did',\n",
       " 'again',\n",
       " 'size',\n",
       " 'just',\n",
       " 'few',\n",
       " 'oil',\n",
       " 'named',\n",
       " 'islands',\n",
       " 'east',\n",
       " 'current',\n",
       " 'top',\n",
       " 'thus',\n",
       " 'once',\n",
       " 'official',\n",
       " 'human',\n",
       " 'england',\n",
       " 'down',\n",
       " 'country',\n",
       " 'australia',\n",
       " 'standard',\n",
       " 'special',\n",
       " 'free',\n",
       " 'century',\n",
       " 'available',\n",
       " '6',\n",
       " '20',\n",
       " 'western',\n",
       " 'see',\n",
       " 'seasons',\n",
       " 'relationship',\n",
       " 'history',\n",
       " 'version',\n",
       " 'return',\n",
       " 'originally',\n",
       " 'go',\n",
       " 'become',\n",
       " '16',\n",
       " 'together',\n",
       " 'association',\n",
       " 'win',\n",
       " 'sold',\n",
       " 'separate',\n",
       " 'point',\n",
       " 'never',\n",
       " 'king',\n",
       " 'case',\n",
       " 'video',\n",
       " 'sea',\n",
       " 'list',\n",
       " '2008',\n",
       " 'seven',\n",
       " 'run',\n",
       " 'record',\n",
       " 'field',\n",
       " 'eight',\n",
       " 'dead',\n",
       " 'san',\n",
       " 'rights',\n",
       " 'making',\n",
       " 'europe',\n",
       " 'de',\n",
       " 'among',\n",
       " 'developed',\n",
       " '50',\n",
       " '2006',\n",
       " 'type',\n",
       " 'studios',\n",
       " 'real',\n",
       " 'how',\n",
       " 'fifth',\n",
       " 'blue',\n",
       " 'various',\n",
       " 'rules',\n",
       " 'open',\n",
       " 'held',\n",
       " 'currently',\n",
       " 'could',\n",
       " '7',\n",
       " '2009',\n",
       " 'union',\n",
       " 'texas',\n",
       " 'southern',\n",
       " 'parts',\n",
       " 'northern',\n",
       " 'live',\n",
       " 'lead',\n",
       " 'hand',\n",
       " 'former',\n",
       " 'areas',\n",
       " 'work',\n",
       " 'said',\n",
       " 'ireland',\n",
       " 'includes',\n",
       " 'great',\n",
       " 'franchise',\n",
       " 'every',\n",
       " 'center',\n",
       " 'side',\n",
       " 'population',\n",
       " 'military',\n",
       " 'john',\n",
       " 'events',\n",
       " 'development',\n",
       " '24',\n",
       " 'others',\n",
       " 'music',\n",
       " 'love',\n",
       " 'london',\n",
       " 'level',\n",
       " 'given',\n",
       " 'food',\n",
       " 'except',\n",
       " 'division',\n",
       " 'speed',\n",
       " 'republic',\n",
       " 'degree',\n",
       " 'color',\n",
       " 'building',\n",
       " 'below',\n",
       " 'approximately',\n",
       " 'vehicle',\n",
       " 'low',\n",
       " 'control',\n",
       " 'cannot',\n",
       " '13',\n",
       " 'uses',\n",
       " 'uk',\n",
       " 'role',\n",
       " 'names',\n",
       " 'mexico',\n",
       " 'get',\n",
       " 'finals',\n",
       " 'aired',\n",
       " 'throughout',\n",
       " 'language',\n",
       " 'goal',\n",
       " 'general',\n",
       " 'feet',\n",
       " 'chain',\n",
       " 'river',\n",
       " 'refers',\n",
       " 'officially',\n",
       " 'network',\n",
       " 'mother',\n",
       " 'michael',\n",
       " 'license',\n",
       " 'led',\n",
       " 'head',\n",
       " 'green',\n",
       " 'energy',\n",
       " 'winning',\n",
       " 'value',\n",
       " 'street',\n",
       " 'square',\n",
       " 'pressure',\n",
       " 'police',\n",
       " 'lower',\n",
       " 'instead',\n",
       " 'france',\n",
       " 'depending',\n",
       " 'county',\n",
       " 'certain',\n",
       " 'trophy']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"This version of the fairy tale character has been very well received by film critics and the public, and is considered one of Disney's most iconic and menacing villains. Besides in the film, the Evil Queen has made numerous appearances in Disney attractions and productions, including not only these directly related to the tale of Snow White, such as Fantasmic!, The Kingdom Keepers and Kingdom Hearts Birth by Sleep, sometimes appearing in them alongside Maleficent from Sleeping Beauty. The film's version of the Queen has also become a popular archetype that influenced a number of artists and non-Disney works.\""
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['validation'].map(lambda items: items['passage']).take(1))).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens are converted to id numbers\n",
    "- based on a created vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(99,), dtype=int64, numpy=\n",
       "array([ 33, 368,   3,   2,   1,   1, 306,  25,  49, 333, 116, 313,  10,\n",
       "        39,   1,   4,   2, 182,   4,   8, 297,  34,   3,   1,  53,   1,\n",
       "         4,   1,   1,   1,   5,   2,  39,   2,   1,   1,  25, 112,   1,\n",
       "         1,   5,   1,   1,   4,   1, 100,  24,  57, 106,   1,   1,   7,\n",
       "         2,   1,   3,   1, 222,  59,   9,   1,   2, 218,   1,   4, 218,\n",
       "         1,   1,  10,   1, 171,   1,   5, 142,   1,   1,  19,   1,   1,\n",
       "         2, 228, 368,   3,   2,   1,  25,  26, 372,   6,   1,   1,  17,\n",
       "         1,   6, 109,   3,   1,   4,   1,   1])>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(next(iter(dataset['validation'].map(lambda items: items['passage']).take(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([b'[UNK]', b'', b'certain', b'county', b'depending', b'france',\n",
       "        b'instead', b'police', b'square', b'street', b'energy', b'green',\n",
       "        b'head', b'led', b'license', b'mother', b'network', b'officially',\n",
       "        b'refers', b'river', b'language', b'throughout', b'aired', b'get',\n",
       "        b'uk', b'13', b'control', b'approximately', b'building', b'color',\n",
       "        b'degree', b'republic', b'speed', b'division', b'except', b'food',\n",
       "        b'level', b'london', b'love', b'john', b'given', b'population',\n",
       "        b'side', b'franchise', b'ireland', b'said', b'great', b'work',\n",
       "        b'hand', b'lead', b'northern', b'southern', b'texas', b'7',\n",
       "        b'could', b'open', b'various', b'blue', b'fifth', b'studios',\n",
       "        b'developed', b'includes', b'11', b'rights', b'trophy',\n",
       "        b'national', b'dead', b'18', b'sea', b'case', b'never', b'point',\n",
       "        b'sold', b'are', b'part', b'association', b'see', b'together',\n",
       "        b'list', b'union', b'order', b'16', b'become', b'will', b'go',\n",
       "        b'relationship', b'history', b'seven', b'20', b'general', b'eight',\n",
       "        b'include', b'6', b'game', b'de', b'available', b'feet', b'free',\n",
       "        b'standard', b'england', b'others', b'both', b'once', b'thus',\n",
       "        b'members', b'version', b'type', b'series', b'current', b'cannot',\n",
       "        b'five', b'directed', b'east', b'following', b'named', b'oil',\n",
       "        b'again', b'over', b'did', b'light', b'or', b'bank', b'card',\n",
       "        b'person', b'constitution', b'million', b'least', b'5', b'sequel',\n",
       "        b'title', b'stars', b'episode', b'2012', b'carry', b'december',\n",
       "        b'rules', b'50', b'early', b'often', b'less', b'when', b'received',\n",
       "        b'seasons', b'currently', b'line', b'august', b'published',\n",
       "        b'their', b'president', b'using', b'would', b'm', b'music',\n",
       "        b'make', b'down', b'november', b'value', b'considered', b'few',\n",
       "        b'2006', b'force', b'novel', b'renewed', b'day', b'result',\n",
       "        b'pressure', b'xbox', b'west', b'long', b'began', b'goal',\n",
       "        b'example', b'much', b'laws', b'no', b'2008', b'drama', b'major',\n",
       "        b'off', b'world', b'later', b'production', b'how', b'months',\n",
       "        b'chain', b'this', b'fourth', b'international', b'days', b'power',\n",
       "        b'single', b'along', b'central', b'size', b'back', b'typically',\n",
       "        b'park', b'total', b'generally', b'15', b'court', b'january',\n",
       "        b'names', b'2011', b'2013', b'left', b'large', b'six', b'against',\n",
       "        b'created', b'house', b'per', b'february', b'member', b'field',\n",
       "        b'country', b'term', b'having', b'many', b'story', b'species',\n",
       "        b'these', b'though', b'english', b'small', b'films', b'every',\n",
       "        b'that', b'times', b'san', b'play', b'i', b'states', b'2010',\n",
       "        b'kingdom', b'white', b'book', b'island', b'3', b'held', b'became',\n",
       "        b'referred', b'even', b'it', b'premiered', b'british', b'its',\n",
       "        b'just', b'produced', b'air', b'usually', b'october', b'making',\n",
       "        b'last', b'fifa', b'still', b'video', b'so', b'body', b'areas',\n",
       "        b'more', b'special', b'due', b'official', b'former', b'largest',\n",
       "        b'place', b'public', b'into', b'since', b'although', b'year',\n",
       "        b'return', b'those', b'red', b'several', b'where', b'king',\n",
       "        b'must', b'do', b'september', b'sometimes', b'events', b'found',\n",
       "        b'located', b'player', b'she', b'league', b'different', b'24',\n",
       "        b'after', b'another', b'ball', b'end', b'original', b'there',\n",
       "        b'team', b'won', b'than', b'best', b'written', b'cup', b'war',\n",
       "        b'km', b'australia', b'local', b'western', b'federal', b'without',\n",
       "        b'season', b'common', b'1', b'been', b'show', b'them', b'company',\n",
       "        b'black', b'players', b'10', b'group', b'america', b'family', b'2',\n",
       "        b'either', b'city', b'within', b'live', b'then', b'water', b'2015',\n",
       "        b'role', b'him', b'based', b'8', b'north', b'song', b'area',\n",
       "        b'other', b'four', b'played', b'home', b'canada', b'parts',\n",
       "        b'third', b'also', b'well', b'released', b'each', b'human',\n",
       "        b'system', b'military', b'before', b'some', b'21', b'below',\n",
       "        b'games', b'own', b'through', b'school', b'2014', b'june', b'uses',\n",
       "        b'2017', b'second', b'right', b'islands', b'episodes', b'may',\n",
       "        b'new', b'does', b'same', b'and', b'european', b'age', b'football',\n",
       "        b'out', b'an', b'during', b'teams', b'being', b'one', b'service',\n",
       "        b'not', b'lower', b'until', b'europe', b'under', b'law', b'period',\n",
       "        b'form', b'center', b'while', b'like', b'television', b'set',\n",
       "        b'2018', b'only', b'three', b'number', b'century', b'time', b'is',\n",
       "        b'between', b'from', b'known', b'if', b'have', b'united', b'among',\n",
       "        b'2009', b'base', b'at', b'the', b'however', b'us', b'use',\n",
       "        b'american', b'range', b'originally', b'in', b'separate', b'who',\n",
       "        b'30', b'of', b'were', b'california', b'commonly', b'april',\n",
       "        b'finals', b'all', b'a', b'used', b'can', b'act', b'he', b'around',\n",
       "        b'final', b'winning', b'12', b'mexico', b'legal', b'her', b'july',\n",
       "        b'made', b'but', b'they', b'record', b'government', b'2016',\n",
       "        b'required', b'first', b'most', b'vehicle', b'4', b'has',\n",
       "        b'university', b'real', b'york', b'main', b'was', b'life',\n",
       "        b'about', b'character', b'low', b'his', b'countries', b'top',\n",
       "        b'similar', b'to', b'south', b'state', b'had', b'michael',\n",
       "        b'because', b'win', b'very', b'be', b'including', b'years',\n",
       "        b'with', b'march', b'which', b'name', b'for', b'two', b'film',\n",
       "        b'any', b'now', b'announced', b'high', b'run', b'called', b'by',\n",
       "        b'as', b'development', b'such', b'death', b'up', b'on', b'people'],\n",
       "       dtype=object),\n",
       " array([  1,   0, 498, 497, 496, 495, 494, 492, 490, 489, 486, 485, 484,\n",
       "        483, 482, 480, 479, 478, 477, 476, 471, 470, 469, 467, 463, 461,\n",
       "        459, 456, 454, 453, 452, 451, 450, 449, 448, 447, 445, 444, 443,\n",
       "        437, 446, 435, 434, 431, 428, 427, 430, 426, 423, 422, 420, 418,\n",
       "        417, 414, 413, 410, 408, 407, 406, 403, 399, 429, 303, 394, 499,\n",
       "        108, 392, 307, 384, 382, 380, 379, 377,  18, 115, 375, 364, 374,\n",
       "        385, 416, 280, 373, 372,  94, 371, 366, 367, 387, 362, 473, 391,\n",
       "        183, 361,  80, 397, 360, 474, 358, 356, 352, 441,  86, 349, 348,\n",
       "        263, 368, 402,  32, 346, 460, 178, 207, 345, 165, 343, 342, 338,\n",
       "         85, 337, 260,  14, 334, 332, 237, 331, 195, 329, 327, 325, 324,\n",
       "        322, 336, 321, 317, 316, 409, 400, 315, 107, 314,  55, 313, 365,\n",
       "        412, 311, 312, 309,  30, 305, 308, 102, 302, 442, 301, 353, 300,\n",
       "        488, 297, 341, 401, 296, 259, 293, 154, 292, 491, 186, 291, 264,\n",
       "        288, 472, 287, 330, 286,  76, 386, 319, 174, 284,  47, 124, 283,\n",
       "        405, 281, 475,  33, 278, 277, 276, 275, 274, 272, 271, 339, 270,\n",
       "        223, 255, 268, 267, 266, 262, 261, 465, 265, 257, 285, 210, 254,\n",
       "        253, 252, 251, 249, 282, 246, 390, 354, 244, 243,  88, 240, 236,\n",
       "        106, 235, 233, 232, 228, 432,  17, 227, 393, 310, 221,  28, 219,\n",
       "        218, 222, 213, 211, 209, 411, 208, 229, 206,  16, 204, 202,  29,\n",
       "        340, 200, 199, 129, 190, 395, 153, 189, 216, 383, 188, 187, 425,\n",
       "         64, 357, 149, 350, 424, 242, 184, 182,  58,  77, 125, 145, 369,\n",
       "        181, 177, 176,  96, 381, 152, 191, 172, 171, 438, 170, 224, 151,\n",
       "         70, 169, 166, 440,  45, 231, 163, 162, 161,  75,  97, 158,  63,\n",
       "        192, 157,  83, 150, 335, 355, 328, 363, 148, 234,  44, 156, 147,\n",
       "         49, 144, 142, 141, 269, 217, 140, 137, 198, 136, 135, 247, 104,\n",
       "        134, 421, 119, 131, 127, 464, 139, 123, 298, 121, 245, 122,  38,\n",
       "        120, 193, 175, 167, 419, 118,  26, 116, 114, 103, 351, 113, 436,\n",
       "        111,  67, 289, 455, 101, 230,  98, 258, 168, 159, 462,  74,  91,\n",
       "        196, 344, 212,  40,  41, 146,  87,   4, 323, 215, 205, 126,  20,\n",
       "         82, 273,  79,  34, 220,  24, 493, 117, 396,  90,  95, 294, 194,\n",
       "        433,  72, 238, 132, 128,  71,  57,  89, 109, 359,  61,   8,  68,\n",
       "         19,  66,  69,  27,  31, 398, 415, 318,  22,   2,  78,  62, 105,\n",
       "         60, 279, 370,   5, 378,  54, 320,   3,  56, 256, 197, 173, 468,\n",
       "         51,   6,  50,  46, 203,  48, 180, 164, 487, 290, 466, 250,  52,\n",
       "        225, 112,  37,  36, 389, 201, 143, 304,  35,  53, 457, 226,  25,\n",
       "        299, 404, 138, 241,  15, 239,  93, 306, 458,  43, 179, 347, 248,\n",
       "          7, 130,  65,  73, 481, 133, 376, 333,  21, 100,  81,  13, 185,\n",
       "         23, 110,  12,  42,  39,  84, 295, 214, 155, 388,  92,  10,   9,\n",
       "        439,  59, 326,  99,  11, 160])]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we can force the lengths to be the same\n",
    "- and we can zero-pad to keep information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 290), dtype=int64, numpy=\n",
       "array([[ 33, 368,   3,   2,   1,   1, 306,  25,  49, 333, 116, 313,  10,\n",
       "         39,   1,   4,   2, 182,   4,   8, 297,  34,   3,   1,  53,   1,\n",
       "          4,   1,   1,   1,   5,   2,  39,   2,   1,   1,  25, 112,   1,\n",
       "          1,   5,   1,   1,   4,   1, 100,  24,  57, 106,   1,   1,   7,\n",
       "          2,   1,   3,   1, 222,  59,   9,   1,   2, 218,   1,   4, 218,\n",
       "          1,   1,  10,   1, 171,   1,   5, 142,   1,   1,  19,   1,   1,\n",
       "          2, 228, 368,   3,   2,   1,  25,  26, 372,   6,   1,   1,  17,\n",
       "          1,   6, 109,   3,   1,   4,   1,   1,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [326,  19,   1,   8,   6,   1, 194,   3, 326, 129,   1,  19,   1,\n",
       "          1,  14,   1,   1,  10,   6,   1,   3,   1,   1,   3, 326,  10,\n",
       "          1,  27,  49,   1,  19,   2, 227,   3,   1,   1,   7,   2,   1,\n",
       "        154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=500,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=20)#100\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or we could do (binary) bag-of-words\n",
    "- back to a \"more traditional\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0.],\n",
       "       [1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=50,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='binary',\n",
    "    pad_to_max_tokens=True)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or we could do (count) bag-of-words\n",
    "- again, a \"more traditional\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[58.,  8.,  5.,  6.,  3.,  2.,  1.,  1.,  1.,  2.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  3.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  2.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [22.,  2.,  4.,  0.,  0.,  2.,  1.,  1.,  0.,  2.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=50,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='count',\n",
    "    pad_to_max_tokens=True)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize_layer_info.get_vocabulary()\n",
    "#next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or we could go straight to TF-IDF\n",
    "- again, a \"more traditional\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[40.19367   ,  5.6922474 ,  3.8372211 ,  4.6309214 ,  2.413208  ,\n",
       "         1.6252658 ,  0.8878175 ,  0.88448596,  1.0743421 ,  2.2482414 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.3355421 ,  0.        ,  1.42611   ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.7497053 ,  5.2276106 ,\n",
       "         1.6735624 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.9272712 ,  1.9522276 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  5.0798264 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  2.1420925 ,  0.        ],\n",
       "       [15.245874  ,  1.4230618 ,  3.069777  ,  0.        ,  0.        ,\n",
       "         1.6252658 ,  0.8878175 ,  0.88448596,  0.        ,  2.2482414 ,\n",
       "         0.        ,  0.        ,  0.        ,  1.3620235 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  4.27833   ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.8501315 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  2.1420925 ,  0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=50,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='tf-idf',\n",
    "    pad_to_max_tokens=True)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to id outputs, but this time with ngrams\n",
    "- first set of columns are 1grams, next set are 2grams, all the way up to ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 485), dtype=int64, numpy=\n",
       "array([[ 38, 493,   3,   2,   1,   1, 405,  29,  64, 447, 152, 414,  11,\n",
       "         48,   1,   4,   2, 237,   4,   8, 395,  39,   3,   1,  68,   1,\n",
       "          4,   1,   1,   1,   5,   2,  48,   2,   1,   1,  29, 148,   1,\n",
       "          1,   5,   1,   1,   4,   1, 131,  28,  74, 138,   1,   1,   7,\n",
       "          2,   1,   3,   1, 286,  76,   9,   1,   2, 282,   1,   4, 282,\n",
       "          1,   1,  11,   1, 225,   1,   5, 184,   1,   1,  21,   1,   1,\n",
       "          2, 296, 493,   3,   2,   1,  29,  30, 499,   6,   1,   1,  19,\n",
       "          1,   6, 142,   3,   1,   4,   1,   1,   1,   1,  10,   1,   1,\n",
       "          1,   1, 169,   1,   1,   1,   1,   1,   1,   1,  32,   1,   1,\n",
       "        183,   1,   1, 146,   1,   1,   1,   1,   1,   1,   1,   1,  15,\n",
       "        137,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,  27,   1,   1,   1,   1,   1,\n",
       "         95,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,  10,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 295,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1]])>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=500,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=5,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"This version of the fairy tale character has been very well received by film critics and the public, and is considered one of Disney's most iconic and menacing villains. Besides in the film, the Evil Queen has made numerous appearances in Disney attractions and productions, including not only these directly related to the tale of Snow White, such as Fantasmic!, The Kingdom Keepers and Kingdom Hearts Birth by Sleep, sometimes appearing in them alongside Maleficent from Sleeping Beauty. The film's version of the Queen has also become a popular archetype that influenced a number of artists and non-Disney works.\">"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['validation'].map(lambda items: items['passage']).take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...\"first set of columns are 1grams, next set are 2grams, all the way up to ngrams\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/606191/convert-bytes-to-a-string\n",
    "len(next(iter(dataset['validation'].map(lambda items: items['passage']).take(1))).numpy().decode(\"utf-8\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 0\n",
    "for ngram in range(5):\n",
    "    length += 99-ngram\n",
    "length    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what sort of lossy information do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'of',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'to',\n",
       " 'is',\n",
       " 'as',\n",
       " 'of the',\n",
       " 'by',\n",
       " 'on',\n",
       " 'for',\n",
       " 'with',\n",
       " 'in the',\n",
       " 'or',\n",
       " 'was',\n",
       " 'it',\n",
       " 'that',\n",
       " 'are',\n",
       " 'from',\n",
       " 'an',\n",
       " 'be',\n",
       " 'at',\n",
       " 'is a',\n",
       " 'which',\n",
       " 'to the',\n",
       " 'not',\n",
       " 'has',\n",
       " 'also',\n",
       " 'have',\n",
       " 'and the',\n",
       " 'states',\n",
       " 'its',\n",
       " 'their',\n",
       " 'united',\n",
       " 'series',\n",
       " 'this',\n",
       " 'one',\n",
       " 'first',\n",
       " 'on the',\n",
       " 'it is',\n",
       " 'they',\n",
       " 'but',\n",
       " 'united states',\n",
       " 'the united',\n",
       " 'other',\n",
       " 'film',\n",
       " 'may',\n",
       " 'new',\n",
       " 'by the',\n",
       " 'two',\n",
       " 'for the',\n",
       " 'is the',\n",
       " 'his',\n",
       " 'season',\n",
       " 'after',\n",
       " 'as a',\n",
       " 'can',\n",
       " 'world',\n",
       " 'as the',\n",
       " 'he',\n",
       " 'the united states',\n",
       " 'been',\n",
       " 'used',\n",
       " 'all',\n",
       " 'her',\n",
       " 'most',\n",
       " 'who',\n",
       " 'when',\n",
       " 'were',\n",
       " 'from the',\n",
       " 'with the',\n",
       " 'only',\n",
       " 'into',\n",
       " 'such',\n",
       " 'american',\n",
       " 'time',\n",
       " 'the first',\n",
       " 'us',\n",
       " 'of a',\n",
       " 'than',\n",
       " 'at the',\n",
       " 'more',\n",
       " 'state',\n",
       " 'known',\n",
       " 'some',\n",
       " 'between',\n",
       " 'in a',\n",
       " 'if',\n",
       " 'she',\n",
       " '2018',\n",
       " 'to be',\n",
       " 'while',\n",
       " 'such as',\n",
       " 'had',\n",
       " '2017',\n",
       " 'there',\n",
       " 'the series',\n",
       " 'it was',\n",
       " 'no',\n",
       " 'with a',\n",
       " 'since',\n",
       " 'however',\n",
       " 'being',\n",
       " 'game',\n",
       " 'years',\n",
       " 'during',\n",
       " 'cup',\n",
       " 'any',\n",
       " 'over',\n",
       " 'both',\n",
       " 'same',\n",
       " 'in the united',\n",
       " 'many',\n",
       " 'known as',\n",
       " 'three',\n",
       " 'for a',\n",
       " 'the same',\n",
       " 'under',\n",
       " 'second',\n",
       " 'called',\n",
       " 'about',\n",
       " 'will',\n",
       " 'law',\n",
       " 'is an',\n",
       " 'where',\n",
       " 'team',\n",
       " 'through',\n",
       " 'up',\n",
       " 'including',\n",
       " 'games',\n",
       " 'would',\n",
       " 'each',\n",
       " 'city',\n",
       " 'use',\n",
       " 'the film',\n",
       " 'these',\n",
       " 'often',\n",
       " 'that the',\n",
       " 'national',\n",
       " 'number',\n",
       " 'in the united states',\n",
       " 'name',\n",
       " 'before',\n",
       " 'one of',\n",
       " 'world cup',\n",
       " 'made',\n",
       " 'system',\n",
       " 'released',\n",
       " 'part',\n",
       " 'well',\n",
       " 'until',\n",
       " 'third',\n",
       " 'then',\n",
       " 'four',\n",
       " 'north',\n",
       " 'the us',\n",
       " 'area',\n",
       " 'based',\n",
       " 'later',\n",
       " 'although',\n",
       " 'out',\n",
       " 'and a',\n",
       " '2015',\n",
       " 'set',\n",
       " 'usually',\n",
       " 'south',\n",
       " 'has been',\n",
       " 'water',\n",
       " 'can be',\n",
       " 'television',\n",
       " 'because',\n",
       " 'within',\n",
       " '2',\n",
       " 'family',\n",
       " 'group',\n",
       " 'york',\n",
       " 'him',\n",
       " 'as well',\n",
       " '10',\n",
       " 'company',\n",
       " 'and is',\n",
       " 'them',\n",
       " 'part of',\n",
       " '2016',\n",
       " 'to a',\n",
       " 'show',\n",
       " 'new york',\n",
       " 'year',\n",
       " 'does',\n",
       " '1',\n",
       " 'federal',\n",
       " 'due',\n",
       " 'one of the',\n",
       " 'war',\n",
       " 'player',\n",
       " 'must',\n",
       " 'which is',\n",
       " 'based on',\n",
       " 'last',\n",
       " 'is not',\n",
       " 'due to',\n",
       " 'day',\n",
       " 'the second',\n",
       " 'high',\n",
       " 'common',\n",
       " 'written',\n",
       " 'won',\n",
       " 'june',\n",
       " 'people',\n",
       " 'original',\n",
       " 'end',\n",
       " 'ball',\n",
       " 'final',\n",
       " 'may be',\n",
       " 'have been',\n",
       " 'following',\n",
       " 'different',\n",
       " 'the world',\n",
       " 'canada',\n",
       " '2014',\n",
       " 'league',\n",
       " 'found',\n",
       " 'sometimes',\n",
       " 'september',\n",
       " 'april',\n",
       " 'major',\n",
       " 'home',\n",
       " 'several',\n",
       " 'red',\n",
       " 'five',\n",
       " 'countries',\n",
       " 'by a',\n",
       " 'around',\n",
       " 'those',\n",
       " 'public',\n",
       " 'well as',\n",
       " 'include',\n",
       " 'as well as',\n",
       " 'they are',\n",
       " 'place',\n",
       " 'march',\n",
       " 'xbox',\n",
       " 'was the',\n",
       " 'into the',\n",
       " 'body',\n",
       " 'so',\n",
       " 'fifa',\n",
       " 'also known',\n",
       " 'october',\n",
       " 'do',\n",
       " 'best',\n",
       " 'played',\n",
       " 'form',\n",
       " 'million',\n",
       " 'right',\n",
       " 'during the',\n",
       " 'commonly',\n",
       " 'america',\n",
       " 'air',\n",
       " 'produced',\n",
       " 'of the united',\n",
       " 'government',\n",
       " 'does not',\n",
       " 'british',\n",
       " 'act',\n",
       " 'premiered',\n",
       " 'football',\n",
       " 'even',\n",
       " 'directed',\n",
       " 'became',\n",
       " '3',\n",
       " 'large',\n",
       " 'island',\n",
       " 'episodes',\n",
       " 'book',\n",
       " 'announced',\n",
       " 'age',\n",
       " 'still',\n",
       " 'players',\n",
       " 'kingdom',\n",
       " '2010',\n",
       " 'service',\n",
       " 'i',\n",
       " 'white',\n",
       " 'typically',\n",
       " 'on a',\n",
       " 'located',\n",
       " 'july',\n",
       " 'also known as',\n",
       " 'after the',\n",
       " '4',\n",
       " 'times',\n",
       " 'number of',\n",
       " 'films',\n",
       " 'referred to',\n",
       " 'referred',\n",
       " 'the new',\n",
       " 'own',\n",
       " 'another',\n",
       " 'small',\n",
       " 'english',\n",
       " 'without',\n",
       " 'though',\n",
       " 'the most',\n",
       " 'species',\n",
       " 'person',\n",
       " 'like',\n",
       " 'life',\n",
       " 'was released',\n",
       " 'story',\n",
       " 'main',\n",
       " 'largest',\n",
       " 'having',\n",
       " 'term',\n",
       " 'song',\n",
       " 'member',\n",
       " 'either',\n",
       " 'between the',\n",
       " 'under the',\n",
       " 'similar',\n",
       " 'per',\n",
       " 'legal',\n",
       " 'as of',\n",
       " 'is also',\n",
       " 'house',\n",
       " 'created',\n",
       " 'against',\n",
       " 'six',\n",
       " 'park',\n",
       " 'california',\n",
       " '2013',\n",
       " 'the two',\n",
       " 'the game',\n",
       " 'school',\n",
       " 'novel',\n",
       " 'more than',\n",
       " 'light',\n",
       " 'january',\n",
       " 'to as',\n",
       " 'fifa world',\n",
       " 'the original',\n",
       " 'directed by',\n",
       " 'court',\n",
       " 'there are',\n",
       " 'members',\n",
       " 'long',\n",
       " '2011',\n",
       " '15',\n",
       " 'this is',\n",
       " 'has a',\n",
       " 'generally',\n",
       " 'total',\n",
       " 'referred to as',\n",
       " 'black',\n",
       " 'back',\n",
       " 'fifa world cup',\n",
       " 'central',\n",
       " 'an american',\n",
       " 'along',\n",
       " 'when the',\n",
       " 'the only',\n",
       " 'teams',\n",
       " 'single',\n",
       " 'power',\n",
       " 'of the united states',\n",
       " 'days',\n",
       " 'are not',\n",
       " 'the show',\n",
       " 'part of the',\n",
       " 'international',\n",
       " 'fourth',\n",
       " 'range',\n",
       " 'order',\n",
       " 'months',\n",
       " 'february',\n",
       " 'united kingdom',\n",
       " 'production',\n",
       " 'off',\n",
       " 'left',\n",
       " 'laws',\n",
       " 'example',\n",
       " 'began',\n",
       " 'based on the',\n",
       " 'and was',\n",
       " '21',\n",
       " '12',\n",
       " 'west',\n",
       " 'result',\n",
       " 'renewed',\n",
       " 'period',\n",
       " 'now',\n",
       " 'force',\n",
       " 'considered',\n",
       " '8',\n",
       " 'university',\n",
       " 'november',\n",
       " 'make',\n",
       " 'm',\n",
       " '11',\n",
       " 'premiered on',\n",
       " 'required',\n",
       " 'president',\n",
       " 'character',\n",
       " '18',\n",
       " 'using',\n",
       " 'published',\n",
       " 'play',\n",
       " 'line',\n",
       " 'end of',\n",
       " 'august',\n",
       " 'the third',\n",
       " 'received',\n",
       " 'of its',\n",
       " 'less',\n",
       " 'known as the',\n",
       " 'early',\n",
       " 'december',\n",
       " 'carry',\n",
       " 'base',\n",
       " 'as an',\n",
       " 'drama',\n",
       " '30',\n",
       " '2012',\n",
       " 'used in',\n",
       " 'the last',\n",
       " 'there is',\n",
       " 'the united kingdom',\n",
       " 'stars',\n",
       " 'european',\n",
       " 'do not',\n",
       " 'title',\n",
       " 'sequel',\n",
       " 'of their',\n",
       " 'death',\n",
       " '5',\n",
       " 'local',\n",
       " 'least',\n",
       " 'it is the',\n",
       " 'used to',\n",
       " 'the end',\n",
       " 'much',\n",
       " 'it is a',\n",
       " 'constitution',\n",
       " 'card',\n",
       " 'very',\n",
       " 'it has',\n",
       " 'in which',\n",
       " 'if the',\n",
       " 'have a',\n",
       " 'bank',\n",
       " 'the ball',\n",
       " 'km',\n",
       " 'from a',\n",
       " 'episode',\n",
       " 'did',\n",
       " 'again',\n",
       " 'television series',\n",
       " 'size',\n",
       " 'just',\n",
       " 'few',\n",
       " 'that is',\n",
       " 'oil',\n",
       " 'named',\n",
       " 'islands',\n",
       " 'east',\n",
       " 'current',\n",
       " 'was a',\n",
       " 'top',\n",
       " 'thus',\n",
       " 'once',\n",
       " 'official',\n",
       " 'human',\n",
       " 'england',\n",
       " 'down',\n",
       " 'country',\n",
       " 'australia',\n",
       " 'at least',\n",
       " 'within the',\n",
       " 'standard',\n",
       " 'special',\n",
       " 'free',\n",
       " 'century',\n",
       " 'available',\n",
       " '6',\n",
       " '20',\n",
       " 'western',\n",
       " 'see',\n",
       " 'seasons',\n",
       " 'relationship',\n",
       " 'history',\n",
       " 'version',\n",
       " 'the term',\n",
       " 'return',\n",
       " 'originally',\n",
       " 'is one',\n",
       " 'go',\n",
       " 'become']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_info.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' this version of the [UNK] [UNK] character has been very well received by film [UNK] and the public and is considered one of [UNK] most [UNK] and [UNK] [UNK] [UNK] in the film the [UNK] [UNK] has made [UNK] [UNK] in [UNK] [UNK] and [UNK] including not only these [UNK] [UNK] to the [UNK] of [UNK] white such as [UNK] the kingdom [UNK] and kingdom [UNK] [UNK] by [UNK] sometimes [UNK] in them [UNK] [UNK] from [UNK] [UNK] the films version of the [UNK] has also become a [UNK] [UNK] that [UNK] a number of [UNK] and [UNK] [UNK] [UNK] [UNK] of the [UNK] [UNK] [UNK] [UNK] has been [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and the [UNK] [UNK] and is [UNK] [UNK] one of [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] in the the film [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] to the [UNK] [UNK] [UNK] [UNK] [UNK] such as [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] of the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] number of [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(vectorize_layer_info)\n",
    "encoding = {k:v for v,k in zip(*vectorize_layer_info.get_weights())}\n",
    "uncoded = b''\n",
    "\n",
    "tmp = vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1)))).numpy()[0]\n",
    "for token in tmp:\n",
    "    uncoded += b' '+encoding[token]\n",
    "uncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_info.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset['validation'].map(lambda items: items['passage']).take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what sort of lossy information do we have if we move our vocabulary from 500 to 5000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(485,), dtype=int64, numpy=\n",
       "array([  38,  493,    3,    2,    1,    1,  405,   29,   64,  447,  152,\n",
       "        414,   11,   48, 1567,    4,    2,  237,    4,    8,  395,   39,\n",
       "          3,    1,   68,    1,    4,    1,    1,    1,    5,    2,   48,\n",
       "          2,    1,  801,   29,  148, 1791, 2317,    5,  934, 4702,    4,\n",
       "       2801,  131,   28,   74,  138,  754,  820,    7,    2,    1,    3,\n",
       "       4850,  286,   76,    9,    1,    2,  282,    1,    4,  282,    1,\n",
       "        969,   11, 3807,  225, 2887,    5,  184, 2892,    1,   21,    1,\n",
       "          1,    2,  296,  493,    3,    2,  801,   29,   30,  499,    6,\n",
       "        917,    1,   19,    1,    6,  142,    3,    1,    4,    1, 2599,\n",
       "          1, 1525,   10,    1,    1,    1,    1,  169,    1,    1,    1,\n",
       "          1,    1,    1,    1,   32, 1588,    1,  183, 2155,    1,  146,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,   15,  137, 1728,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1, 2017,   27,    1,    1,    1,\n",
       "          1,    1,   95,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1, 1427,    1, 1525,   10,    1,    1, 2991,    1, 4696,    1,\n",
       "          1,    1,    1,    1, 1067,  295,    1,    1,    1,    1,    1,\n",
       "       2754,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1, 2843,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1, 2754,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1, 1324,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1])>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 5000\n",
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=5,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).take(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'this version of the [UNK] [UNK] character has been very well received by film critics and the public and is considered one of [UNK] most [UNK] and [UNK] [UNK] [UNK] in the film the [UNK] queen has made numerous appearances in disney attractions and productions including not only these directly related to the [UNK] of snow white such as [UNK] the kingdom [UNK] and kingdom [UNK] birth by sleep sometimes appearing in them alongside [UNK] from [UNK] [UNK] the films version of the queen has also become a popular [UNK] that [UNK] a number of [UNK] and [UNK] works [UNK] version of of the [UNK] [UNK] [UNK] [UNK] has been [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and the the public [UNK] and is is considered [UNK] one of [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] in the the film film the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] related to to the [UNK] [UNK] [UNK] [UNK] [UNK] such as [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the films [UNK] version of of the [UNK] [UNK] has also [UNK] become a [UNK] [UNK] [UNK] [UNK] [UNK] a number number of [UNK] [UNK] [UNK] [UNK] [UNK] version of the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] in the film [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] version of the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] a number of [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] '"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(vectorize_layer_info)\n",
    "encoding = {k:v for v,k in zip(*vectorize_layer_info.get_weights())}\n",
    "uncoded = b''\n",
    "\n",
    "tmp = vectorize_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1))))\n",
    "for i,token in enumerate(tmp.numpy()[0]):\n",
    "    #print(i, encoding[token])\n",
    "    uncoded += encoding[token]+b' '\n",
    "uncoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at ngrams and 1grams\n",
    "- both are in there if their counts are sufficiently large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['superfecundation refers to the fertilization',\n",
       " 'the fertilization of two separate',\n",
       " 'fertilization of two separate ova',\n",
       " 'national basketball association',\n",
       " 'heteropaternal superfecundation',\n",
       " 'government of the united states',\n",
       " 'the united states constitution',\n",
       " 'superfecundation refers to the',\n",
       " 'refers to the fertilization of',\n",
       " 'arrival to and departure from']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorize_layer_info.get_vocabulary(), key=lambda x: -len(x))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'superfecundation' in vectorize_layer_info.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting this into Keras with `tf.one_hot`\n",
    "- We want the id version the best for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_info = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=2,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None)\n",
    "\n",
    "vectorize_layer_info.adapt(dataset['validation'].map(lambda items: items['passage']).batch(64), reset_state=True)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(vectorize_layer_info)\n",
    "# tf.one_hot?\n",
    "model.add(tf.keras.layers.Lambda(lambda inputs: tf.one_hot(inputs, depth=max_features, axis=-1)))\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 197, 5000), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1)))\n",
    "model(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tmp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_info(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting this into Keras with `tf.keras.layers.Embedding`\n",
    "- we like the id version the best for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(vectorize_layer_info)\n",
    "#tf.keras.layers.Embedding?\n",
    "embedding_dimension = 2\n",
    "model.add(tf.keras.layers.Embedding(input_dim=max_features, output_dim=embedding_dimension))\n",
    "model.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 197, 2), dtype=float32, numpy=\n",
       "array([[[ 0.00167499,  0.02314531],\n",
       "        [-0.03217977,  0.0374197 ],\n",
       "        [-0.03087872, -0.03238895],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.02488396, -0.00570374],\n",
       "        [ 0.04216411, -0.03048191],\n",
       "        [-0.03403851,  0.01894305],\n",
       "        [ 0.04409349, -0.00722116],\n",
       "        [ 0.04811498,  0.02344055],\n",
       "        [ 0.03362997, -0.01428165],\n",
       "        [-0.0391606 ,  0.03464741],\n",
       "        [ 0.00127979, -0.03865837],\n",
       "        [ 0.00763844,  0.00998465],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [-0.01558869, -0.00568979],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [ 0.00833434, -0.01324867],\n",
       "        [ 0.02063389, -0.04137963],\n",
       "        [ 0.03562958, -0.01578074],\n",
       "        [-0.03087872, -0.03238895],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.01582806,  0.048185  ],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03614658,  0.02687259],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [ 0.00127979, -0.03865837],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.01802515,  0.03481271],\n",
       "        [ 0.04216411, -0.03048191],\n",
       "        [ 0.03599543,  0.01315911],\n",
       "        [-0.0293191 , -0.04774413],\n",
       "        [ 0.01799304,  0.02924042],\n",
       "        [-0.03614658,  0.02687259],\n",
       "        [-0.04027077,  0.00296595],\n",
       "        [-0.01794422,  0.03941495],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [ 0.04847783,  0.03615388],\n",
       "        [ 0.00079082,  0.0128536 ],\n",
       "        [ 0.02884175, -0.03847378],\n",
       "        [-0.0288692 ,  0.01781924],\n",
       "        [ 0.02994906,  0.02667482],\n",
       "        [-0.00388772,  0.02576709],\n",
       "        [-0.03275756,  0.02701065],\n",
       "        [-0.0497237 ,  0.01707743],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03087872, -0.03238895],\n",
       "        [ 0.02323612, -0.0378947 ],\n",
       "        [-0.0035725 , -0.00718131],\n",
       "        [ 0.04855764, -0.00969042],\n",
       "        [ 0.04273984,  0.04502885],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [ 0.02459928, -0.00116939],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [ 0.02459928, -0.00116939],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04339157, -0.01315445],\n",
       "        [-0.0391606 ,  0.03464741],\n",
       "        [-0.04983259,  0.0379364 ],\n",
       "        [ 0.01628213,  0.00826252],\n",
       "        [ 0.03180598,  0.00400171],\n",
       "        [-0.03614658,  0.02687259],\n",
       "        [ 0.04800623, -0.03248328],\n",
       "        [ 0.040792  , -0.01047354],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.01685442, -0.00057688],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [-0.00662358, -0.03418969],\n",
       "        [-0.03217977,  0.0374197 ],\n",
       "        [-0.03087872, -0.03238895],\n",
       "        [-0.00092924, -0.03910672],\n",
       "        [ 0.01802515,  0.03481271],\n",
       "        [ 0.04216411, -0.03048191],\n",
       "        [-0.00493891, -0.00134112],\n",
       "        [ 0.02554703, -0.00987013],\n",
       "        [ 0.01462201,  0.00393028],\n",
       "        [ 0.00599156, -0.04081571],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.04861429,  0.02953437],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.01462201,  0.00393028],\n",
       "        [ 0.04134288, -0.04519404],\n",
       "        [-0.03087872, -0.03238895],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03237952,  0.00126277],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.00843705,  0.02558139],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.02166252, -0.03369286],\n",
       "        [-0.00310703, -0.04128674],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.00201548,  0.00095669],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.0217399 ,  0.01600106],\n",
       "        [ 0.01556439, -0.03360827],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.03386335,  0.01416873],\n",
       "        [-0.048724  , -0.04551754],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.01564256,  0.04305336],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.00979861,  0.02205576],\n",
       "        [-0.02751373, -0.03403919],\n",
       "        [-0.00084888,  0.04188944],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.02493427,  0.04531579],\n",
       "        [-0.0176693 ,  0.04609468],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.01879392,  0.01767529],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.005041  ,  0.01152683],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.00691213, -0.03751398],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.02166252, -0.03369286],\n",
       "        [-0.00310703, -0.04128674],\n",
       "        [-0.04058408,  0.01744194],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.01188963, -0.04167945],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [ 0.00702522, -0.01494645],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.02215498,  0.04693696],\n",
       "        [-0.03770357,  0.00260346],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025],\n",
       "        [-0.04146794,  0.01615025]]], dtype=float32)>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1)))\n",
    "model(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Widths are the same size for each observation\n",
    "## But don't mean anything consistent across observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_info(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_info.get_vocabulary()[443]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Widths are the same size for each observation\n",
    "## But don't mean anything consistent across observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Lambda(lambda inputs: tf.math.reduce_mean(inputs, axis=1)))\n",
    "# GlobalAveragePooling1D\n",
    "# https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "model.compile()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if tokens are not words?\n",
    "- i.e., they are substrings of characters in a sentence string?\n",
    "- these are called \"word pieces\"\n",
    "\n",
    "\n",
    "- I explored quite a few ways to execute this idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first try:\n",
    "# None; list -- None means full string is token; list is not a tf function\n",
    "\n",
    "# second try: https://www.tensorflow.org/api_docs/python/tf/strings/bytes_split\n",
    "# split=tf.strings.bytes_split, \n",
    "# ...almost works, but TextVectorization assumes ngrams are getting separated by space ' '\n",
    "\n",
    "# third try: https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
    "# not going to be supported...\n",
    "\n",
    "# fourth try: there is a new subword encoder\n",
    "# https://blog.tensorflow.org/2019/06/introducing-tftext.html\n",
    "# https://dzlab.github.io/nlp/2019/12/25/tensorflow-text-intro/\n",
    "# https://www.tensorflow.org/tutorials/tensorflow_text/intro\n",
    "# ! pip install -q tensorflow-text\n",
    "import tensorflow_text as text\n",
    "#text.WordpieceTokenizer\n",
    "\n",
    "# firth try: keras preprocess\n",
    "# https://towardsdatascience.com/how-to-preprocess-character-level-text-with-keras-349065121089\n",
    "#tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=500, char_level=True, oov_token='[UNK]')\n",
    "#tokenizer.fit_on_texts(dataset['validation'].map(lambda items: items['passage']).batch(64))\n",
    "# https://stackoverflow.com/questions/61445913/how-do-i-preprocess-and-tokenize-a-tensorflow-csvdataset-inside-the-map-method\n",
    "# -- didn't pursue this because it doesn't immediately work with a TF Dataset object \n",
    "\n",
    "\n",
    "# conclusion: either the second or the fourth \n",
    "# - second IS fine -- just need to put it back together without spaces\n",
    "# - fourth IS interesting -- just need to provide the `vocab_lookup_table`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (2.3.0)\n",
      "Collecting tensorflow<2.4,>=2.3.0\n",
      "  Downloading tensorflow-2.3.2-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165.2 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.35.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (3.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (2.4.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow<2.4,>=2.3.0->tensorflow-text) (49.6.0.post20200925)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.22.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.3.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.1.0)\n",
      "Installing collected packages: tensorflow-estimator, numpy, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.4.0\n",
      "    Uninstalling tensorflow-2.4.0:\n",
      "      Successfully uninstalled tensorflow-2.4.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-serving-api 2.4.0 requires tensorflow<3,>=2.4.0, but you'll have tensorflow 2.3.2 which is incompatible.\n",
      "matplotlib 3.3.1 requires certifi>=2020.06.20, but you'll have certifi 2019.11.28 which is incompatible.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed numpy-1.19.2 tensorflow-2.3.2 tensorflow-estimator-2.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I finally settled on using BERT's vocabulary implementation of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/text/issues/27\n",
    "# https://github.com/google-research/bert\n",
    "# https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\n",
    "# https://medium.com/tensorflow/introducing-tf-text-438c8552bd5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/14676265/how-to-read-a-text-file-into-a-list-or-an-array-with-python\n",
    "path = '/Users/gck8gd/Documents/courses/SYS_6016_DeepLearning/L111/'\n",
    "file = 'bert-base-uncased-vocab.txt'\n",
    "vocab_file = open(path+file, \"r\")\n",
    "vocab = vocab_file.read().split('\\n')#.readlines()\n",
    "vocab_file.close()\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {path+file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We set up our Vocab Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 1\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets, lookup_key_dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {i:token for token,i in zip(vocab, indices.numpy())}\n",
    "# encoding[6677]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We set up the two Tokenizers for comparison\n",
    "- `text.WhitespaceTokenizer()`\n",
    "- `text.WordpieceTokenizer(table)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece = text.WordpieceTokenizer(table)\n",
    "whitespace = text.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1)))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usual `Whitespace` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace.tokenize_with_offsets(tf.strings.lower(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The new `Wordpiece` style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = whitespace.tokenize_with_offsets(tf.strings.lower(tmp))\n",
    "wordpiece.tokenize_with_offsets(tmp2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece.tokenize_with_offsets(['this', 'version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to convert to a flat representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/text/issues/155\n",
    "\n",
    "@tf.function\n",
    "def wp_tokenizer(x): \n",
    "    wp = wordpiece.tokenize_with_offsets(x[0])\n",
    "    return (wp[0].merge_dims(-2, -1), \n",
    "           (wp[1]+tf.expand_dims(x[1], -1)).merge_dims(-2, -1), \n",
    "           (wp[2]+tf.expand_dims(x[2], -1)).merge_dims(-2, -1))\n",
    "\n",
    "wp_tokenizer(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordpiece_layer_info = lambda item: wp_tokenizer(whitespace.tokenize_with_offsets(tf.strings.lower(item)))\n",
    "\n",
    "tmp3 = wordpiece_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(1).take(1))))\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncoded = ''\n",
    "for token in tmp3[0][0].numpy():\n",
    "    uncoded += encoding[token]+' '\n",
    "uncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncoded.replace(' ##', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing this as a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two observations (batch)\n",
    "tmp = wordpiece_layer_info(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2).take(1))))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp[0][0]),len(tmp[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single observation\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix into consistent shapes (for inputting to Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/34793\n",
    "tmp[0].to_tensor(shape=[None, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including Offset Information(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/stack\n",
    "tf.stack(tmp).to_tensor(shape=[3, None, 200])[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/stack\n",
    "tf.stack(tmp).to_tensor(shape=[3, None, 200])[1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/stack\n",
    "tf.stack(tmp).to_tensor(shape=[3, None, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting these up as Keras layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "max_sentence_length = 200\n",
    "model.add(tf.keras.layers.Lambda(lambda inputs: tf.stack(wordpiece_layer_info(inputs)), name=\"WordPiece\"))\n",
    "model.add(tf.keras.layers.Lambda(lambda inputs: inputs.to_tensor(shape=[3, None, max_sentence_length]), name=\"FixedWidth\"))\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/slice\n",
    "words = tf.keras.layers.Lambda(lambda inputs: inputs[0,:,:], name='tokens')(model.outputs[0])\n",
    "embedding_dimension = 2\n",
    "words_embedded = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=embedding_dimension)(words)\n",
    "\n",
    "locations = tf.keras.layers.Lambda(lambda inputs: inputs[1:,:,:], name='offsets')(model.outputs[0])\n",
    "\n",
    "#https://github.com/keras-team/keras/issues/3557\n",
    "wp_model = tf.keras.Model(inputs=model.inputs, outputs=[words_embedded,locations])\n",
    "\n",
    "out = wp_model(next(iter(dataset['validation'].map(lambda items: items['passage']).batch(2))))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(wp_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
