{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Can We Train Intelligent Agents?\n",
    "\n",
    "Let's anchor ourselves in the performance scale from Prof. Nguyen's video:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](range.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithm\n",
    "- Gradient Free\n",
    "\n",
    "### Greatest Sampling Requirements\n",
    "- Approximate Exhaustive Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1803.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](range.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL - policy gradients (PG)\n",
    "- algorithm focused *on-policy* parameters\n",
    "\n",
    "### Great Sampling Requirements\n",
    "- sampling inefficient: \"needs to explore the game for a very long time before it can make significant progress\"\n",
    "- Ideas for improved speed \n",
    "    - **Initialize to a decent policy**\n",
    "        - \"if you already have a reasonably good policy (e.g., hardcoded) ... train the neural network to imitate it before using policy gradients\"\n",
    "    - **Make rewards less sparse**\n",
    "        - CartPole-v1: \"add negative rewards proportional to the poleâ€™s angle [to] make the rewards much less sparse and speed up training\"\n",
    "\n",
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `conda install -c conda-forge gym`\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hands-On ML textbook has great chapters walking through different topics (such as RL)\n",
    "    - (if you haven't already seem some of it)\n",
    "    - I'm adapting much of that material here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Hands-On ML book...\n",
    "\n",
    "# if you're rendering with `env.render()` like below\n",
    "# close before starting new env; otherwise, not needed\n",
    "env.close()\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "obs = env.reset()\n",
    "obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Environment `obs` contains the following:*\n",
    "1. horizontal position (0.0 = center)\n",
    "2. velocity (positive means right)\n",
    "3. angle of the pole (0.0 = vertical)\n",
    "4. angular velocity (positive means right)\n",
    "\n",
    "*Actions we can take:*\n",
    "\n",
    "0. Left\n",
    "1. Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left/right: 0/1\n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sweet video game quality graphics available with `env.render()`\n",
    "    - **Make sure you CLOSE THE WINDOW WITH `env.close()`**\n",
    "        - *or the window won't close until the kernel restarts/dies*\n",
    "\n",
    "`env.close()` only closes the window, not the \"environment session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! \n",
    "- just run this a few times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0 # accelerate right\n",
    "obs, reward, done, info = env.step(action)\n",
    "obs\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Game ends a little early, I would say, sadly...\n",
    "    - Or maybe it closes once you're past the point of \"no return\" and can't save it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Close window with `env.close()` when you're done \n",
    "- BEFORE making new `env = gym.make(\"CartPole-v1\")`\n",
    "    - or you'll lose your pointer to the window\n",
    "    - and won't be able to close it without a kernel restart/shutdown\n",
    "  \n",
    "  \n",
    "`env.close()` only closes the window, not the \"environment session\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Policy!\n",
    "- go in the direction to poll is tipping (to save it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    \n",
    "    '''go in the direction to poll is tipping (to save it!)'''\n",
    "    \n",
    "    angle = obs[2]\n",
    "    \n",
    "    return 0 if angle < 0 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let's run our agent a bunch of times to see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import numpy as np\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        # added commented code \n",
    "        # https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb\n",
    "        #sleep(.01)\n",
    "        #env.render()\n",
    "        \n",
    "        # let's let the game play all the way out\n",
    "        done = obs[2]<-np.pi/2 or obs[2]>np.pi/2        \n",
    "        if done:\n",
    "            #sleep(.1)\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's build a model the predicts L/R according to the `obs` environment\n",
    "- and beat our simple agent above\n",
    "- **But notice: our parameters are set to *make decisions***\n",
    "    - this is why this is an **on-policy** algorithm\n",
    "    - it needs to learn how to react to all environmental conditions\n",
    "    - ***[as opposed to just learning if something is good or bad]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# https://keras.io/api/utils/model_plotting_utils/\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE ALGORITHM\n",
    "- remember every move we made\n",
    "- and the gradient \n",
    "    - so we can make those moves more or less likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "\n",
    "def gradient_to_REINFORCE_move(obs, model, loss_fn):  \n",
    "    \n",
    "    '''We move in some direction:\n",
    "    \n",
    "       What's the gradient of the parameters that make\n",
    "       it even MORE likely to move in that direction?\n",
    "       \n",
    "       Later, when we decide if we like the move or not,\n",
    "       we can follow the gradient to REINFORCE the move \n",
    "       or, or the negative of the gradient to dscount it\n",
    "    '''\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        prob_right = model(obs[np.newaxis])\n",
    "        \n",
    "        # what we did: just a chance according to the currently predicted probability\n",
    "        bool_right = tf.random.uniform([1, 1]) < prob_right\n",
    "        action = tf.cast(bool_right, tf.float32)\n",
    "        \n",
    "        # we \"want\" the predicted prob to even better reflect what we did\n",
    "        loss = tf.reduce_mean(loss_fn(action, prob_right))\n",
    "\n",
    "    # so how do the model variables change to make the predicted probs\n",
    "    # better match what we did?  Record these adjustments\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "                                     \n",
    "    return action, grads\n",
    "                        \n",
    "                                       \n",
    "def play_one_step_get_AND_gradient_to_REINFORCE_move(env, obs, model, loss_fn):\n",
    "                                           \n",
    "    '''get `obs`; take an action in `env`; record gradients to REINFORCE move'''\n",
    "    \n",
    "    action,grads = gradient_to_REINFORCE_move(obs, model, loss_fn)                                   \n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "                                       \n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play sessions management\n",
    "\n",
    "#### `play_an_episode`\n",
    "1. re-apply `play_one_step_get_AND_gradient_to_REINFORCE_move`\n",
    "    - updating `env` state; recording REINFORCE gradients; and how long we've stayed \"alive\"\n",
    "    - until we hit *done*\n",
    "    - \n",
    "\n",
    "#### `play_multiple_episodes`\n",
    "2. repeat a series of the above *full game* runs\n",
    "    - the `reward` for each game is how long we stayed \"alive\" in that game\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sneak Peak\n",
    "\n",
    "Each set of gradients attached to a game with a `reward`\n",
    "- following the gradients \"down\" to reduce \"loss\" makes the predicted probabilities closer to the actions we took\n",
    "- so we should do ths for the \"good\" games\n",
    "- and maybe the opposite for the \"bad\" games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "def play_an_episode(env, n_max_steps, model, loss_fn):\n",
    "\n",
    "    '''Play moves until game is finished,\n",
    "       and keep the gradient that would \n",
    "       REINFORCE each move, and keep the\n",
    "       rewards measured during the game\n",
    "    '''\n",
    "    \n",
    "    current_rewards = []\n",
    "    current_grads = []\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        obs, reward, done, grads =\\\n",
    "        play_one_step_get_AND_gradient_to_REINFORCE_move(env, obs, model, loss_fn)\n",
    "        current_rewards.append(reward)\n",
    "        current_grads.append(grads)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return current_rewards,current_grads\n",
    "\n",
    "        \n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "\n",
    "    '''Play several games, and collect for each game,\n",
    "       the gradients that would REINFORCE each move\n",
    "       and the measured rewards for each move made\n",
    "    '''\n",
    "\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        episode_rewards,episode_grads = play_an_episode(env, n_max_steps, model, loss_fn)\n",
    "        all_rewards.append(episode_rewards)\n",
    "        all_grads.append(episode_grads)\n",
    "        \n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Details on `Rewards`\n",
    "- 1 point for each time step\n",
    "- plus discounted \n",
    "    - if we're still alive later, then maybe the earlier moves were even better than we thought!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *slightly* reworked from book\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    \n",
    "    '''For each completed game, address the credit assignment \n",
    "       problem by applying the discounted rewards strategy:\n",
    "    '''\n",
    "    \n",
    "    discounted_rewards = np.array(rewards)\n",
    "    # work from the end: multiply by the discount factor and add to the previous step\n",
    "    # in this way we accumulate the power decayed scores through the earlier moves\n",
    "    for step in range(len(rewards)-2, -1, -1):\n",
    "        discounted_rewards[step] += discounted_rewards[step+1] * discount_factor\n",
    "        \n",
    "    return discounted_rewards\n",
    "\n",
    "discount_rewards([10, 0, -50], discount_factor=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good or Bad game?\n",
    "\n",
    "- Just normalize the scores: subtract mean an divide by standard deviation\n",
    "- Above average is REINFORCED (proportionally to how good a game is relatively)\n",
    "    - and we do the opposite for below average (proportionally to how bad a game is relatively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "def normalize_rewards(all_rewards):\n",
    "    \n",
    "    '''Subtract and then divide each reward by \n",
    "       the global mean and standard deviation \n",
    "       of all rewards across all games played\n",
    "    '''\n",
    "\n",
    "    flat_rewards = np.concatenate(all_rewards)\n",
    "    all_rewards_mean = flat_rewards.mean()\n",
    "    all_rewards_std = flat_rewards.std()\n",
    "    return [(single_episode_rewards-all_rewards_mean)/all_rewards_std\n",
    "            for single_episode_rewards in all_rewards]\n",
    "\n",
    "discounted_rewards = discount_rewards([10, 0, -50], discount_factor=0.8)\n",
    "normalize_rewards([discounted_rewards])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And here we just put together the `discount_rewards` and `normalize_rewards`\n",
    "    - so we can just call it all at once we've collected all the games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    \n",
    "    '''Use the functions above to tranform\n",
    "       rewards to discounted rewards and\n",
    "       normalize the discounted rewards\n",
    "    '''\n",
    "    \n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    \n",
    "    return normalize_rewards(all_discounted_rewards)\n",
    "    \n",
    "    \n",
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE algorithm at work\n",
    "- One \"epoch\" (of which we'll do 150)\n",
    "    1. Play 10 games (i.e., episodes)\n",
    "    2. discount/normalize each actions score; and, relatively update parameters with the associated gradients \n",
    "        - gradients REINFORCE the actions\n",
    "            - so we follow the gradients for actions with \"good\"\n",
    "                - and go in the opposite direction of the gradient for actions with \"bad\" scores\n",
    "        - This is accomplished by using a *weighted average* of each parameters gradients based on the associated *relative* discounted/normalized action scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "n_gradient_updates = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_actions_per_episodes = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "for index in range(n_gradient_updates):\n",
    "    \n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, \n",
    "                                                    n_max_actions_per_episodes, \n",
    "                                                    model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    \n",
    "    # 1. Each move in each game now has a normalized discounted score\n",
    "    # 2. We have the gradient that REINFORCES each move\n",
    "    # 3. We weight each gradient by the move's normalized discounted score\n",
    "    #    - positive scores REINFORCE the move: \n",
    "    #      i.e., the direction of the gradient REINFORCES the move\n",
    "    #    - negative scores SUPRESS the move:\n",
    "    #      i.e., the negative direction of the gradient SUPRESSES the move\n",
    "    # 4. We average the gradients for each `model.trainable_variables`\n",
    "    \n",
    "    # We do it this way because of how we ended\n",
    "    # up storing lists of gradients and scores\n",
    "    trainable_variables_mean_grad = []\n",
    "    for trainable_variables_index in range(len(model.trainable_variables)):\n",
    "        grad_mean = tf.reduce_mean([\n",
    "            \n",
    "        final_reward*all_grads[episode_index][action_index][trainable_variables_index]\n",
    "            \n",
    "        for episode_index,final_rewards in enumerate(all_final_rewards)\n",
    "            for action_index,final_reward in enumerate(final_rewards)\n",
    "            \n",
    "                                    ], axis=0)\n",
    "        \n",
    "        trainable_variables_mean_grad.append(grad_mean)\n",
    "\n",
    "    optimizer.apply_gradients(zip(trainable_variables_mean_grad, \n",
    "                                  model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And we've seen this next code below already up above before - just plays/shows the games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = int(np.random.random() < model(obs[np.newaxis]))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        \n",
    "        sleep(.01)\n",
    "        env.render()\n",
    "        done = obs[2]<-np.pi/2 or obs[2]>np.pi/2 \n",
    "        if done:\n",
    "            sleep(.1)\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "    \n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm V1\n",
    "\n",
    "- Theoretical/Pedagogical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Markov decision processes (MDPs)\n",
    "### Hands-On ML Textbook Example:\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1808.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "\n",
    "\n",
    "\n",
    "$\\underline{\\textbf{FOR EACH } s}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{Value^{optimal}(State_s)}_{V^*(s)} = & {} \\sum_{s'} \n",
    "\\underbrace{Pr(State_s\\rightarrow State_{s'}| Action^{optimal})}_{ p_{a}(s\\rightarrow s')} \\,\\times \\left[ \\underbrace{Reward(State_s\\rightarrow State_{s'}| Action^{optimal})}_{$_{a}(s\\rightarrow s')}\n",
    "+  \\underbrace{\\overset{\\huge \\text{FUTURE}}{\\gamma Value^{optimal}(State_{s'})}}_{\\gamma V^*(s')} \\right]\n",
    "\\\\\n",
    "V^*(s) = & {} \\underset{a}{\\max} \\sum_{s'} p_{a}(s\\rightarrow s') \\times \\left[ $_{a}(s\\rightarrow s') \\, + \\gamma V^*(s') \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(with discount factor $\\gamma$)\n",
    "\n",
    "#### Value Iteration Algorithm (Dynamic Programming)\n",
    "- Initialize $\\text{\"}Value^{optimal}(State_s)\\text{\"}=0$ for ALL $s$; start at step $k=0$; and iterate:\n",
    "    \n",
    "$\\underline{\\textbf{FOR EACH } s}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\text{\"}Value^{optimal}(State_s)\\text{\"}}}_{\\text{At STEP } k+1} \\longleftarrow & {} \\sum_{s'} \n",
    "\\underbrace{Pr(State_s\\rightarrow State_{s'}| Action^{optimal})}_{\\text{do you know this?}} \\,\\times \\left[ \\underbrace{Reward(State_s\\rightarrow State_{s'}| Action^{optimal})}_{\\text{do you know this?}}\n",
    "\\, + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\text{\"}Value^{optimal}(State_{s'})\\text{\"}}}_{\\text{At STEP } k} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\longrightarrow$ Will converge to *true* $Value^{optimal}(State_s) = V^*(s)$ for ALL $s$\n",
    "\n",
    "(by choosing actions with the highest probability of going to the currently best available state)\n",
    "\n",
    "### State-Action Values (Q-values)\n",
    "\n",
    "(...add **ACTIONS** into the tracking mix...)\n",
    "\n",
    "#### Q-Value Iteration Algorithm\n",
    "\n",
    "- Initialize $\\hat Q(State_s, \\text{ACTION}_a)=0$ for ALL $s$; start at step $k=0$; and iterate:\n",
    "    \n",
    "$\\underline{\\textbf{FOR EACH } s \\text{ and } a}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\hat Q(State_s, \\text{ACTION}_a)}}_{\\text{At STEP } k+1} \\longleftarrow & {} \\sum_{s'} \n",
    "\\underbrace{Pr(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{\\text{do you know this?}} \\,\\times \\left[ \\underbrace{Reward(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{\\text{do you know this?}}\n",
    "\\, + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_{s'}, \\text{ACTION}^{optimal})}}_{\\text{At STEP } k} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\longrightarrow$ Will converge to *true* $Q(State_s, \\text{ACTION}_a) = Q(s, a) $ for ALL $s$ and $a$\n",
    "\n",
    "(by trying and updating things for each of the actions available at each state, not just the \"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hands-On ML Textbook Example:\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1808.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Hands-On ML textook image above\n",
    "\n",
    "# shape=[s, a, s']\n",
    "# to get States:\n",
    "# (s_0,s_1,s_2): transition_probabilities[s]\n",
    "# to get Actions in a State\n",
    "# (s_0:a_0,s_0:a_1,s_0:a_2): transition_probabilities[s_0][a]\n",
    "# to get Move Probabilities for an Action in a State\n",
    "# ( Pr(s_0:a_0->s_0), Pr(s_0:a_0->s_1), Pr(s_0:a_0->s_2) ): transition_probabilities[s_0][a_0][s']\n",
    "\n",
    "transition_probabilities = [[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "                            [[0.0, 1.0, 0.0], None,            [0.0, 0.0, 1.0]],\n",
    "                            [ None,           [0.8, 0.1, 0.1], None           ]]\n",
    "# shape=[s, a, s']\n",
    "rewards = [[[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "           [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "           [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) initialize Q-Values to 0 for all possible actions\n",
    "# s X a: Q_values[state, actions\n",
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  \n",
    "\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) FOR ALL s and a\n",
    "\n",
    "gamma = 0.9 # the discount factor\n",
    "\n",
    "# iteration is \"k\" above\n",
    "for iteration in range(1000):\n",
    "    \n",
    "    # current state\n",
    "    Q_prev = Q_values.copy()\n",
    "    # to be updated with reward and above \"curent\" state\n",
    "    Q_values[~np.isinf(Q_values)] = 0\n",
    "    # try from each possible state\n",
    "    for s in range(3):\n",
    "        # try all available actions at the state\n",
    "        for a in possible_actions[s]:\n",
    "            \n",
    "            Q_values[s, a] = 0\n",
    "            # the states we arrive at with probability `transition_probabilities[s][a][sp]`\n",
    "            for sp in range(3):\n",
    "                Q_values[s, a] += transition_probabilities[s][a][sp] \\\n",
    "                                  * ( rewards[s][a][sp] + gamma*np.max(Q_prev[sp]) )\n",
    "                                     #reward for move     discounted OPTIMAL value at the state just moved to\n",
    "\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should we do in State_1 ?\n",
    "# Action_0 or Action_2 ?\n",
    "\n",
    "Q_values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm V2\n",
    "\n",
    "- off-policy algorithm\n",
    "    - Îµ-greedy policy\n",
    "    - Q-Learning using an exploration function\n",
    "\n",
    "\n",
    "### N/A Sampling Requirements\n",
    "\n",
    "- Theoretical/Impractical/Untractable\n",
    "\n",
    "\n",
    "### Unknown MDP Transtion Probabilities\n",
    "- You may take Actions and see Rewards; but\n",
    "    - the results of your actions are (intially) unknown to you\n",
    "\n",
    "#### ~Temporal Difference Learning (TD Learning) Algorithm~ Q-Value Iteration algorithm\n",
    "\n",
    "\n",
    "- Initialize $\\hat Q(State_s, \\text{ACTION}_a)=0$ for ALL $s$; set $k=0$; choose an initial $State_s$; and iterate:\n",
    "    \n",
    "\n",
    "$\\underline{ \\text{Take a } \\overset{*random*}{\\text{ACTION}}_a \\text{ to arrive at the next state } s'}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\hat Q(State_s, \\overset{*random*}{\\text{ACTION}}_a)}}_{\\text{At STEP } k+1} \\longleftarrow & {} \n",
    "\\; \\left(1-\\overset{\\text{chosen}}{w^{update}_{speed}}\\right)\\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_s, \\overset{random}{\\text{ACTION}})}}_{\\text{At STEP } k} \\; +\\\\\n",
    "& {}  \n",
    "\\underbrace{ \\sum_{s'} Pr(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{\\text{Because You Do Not Know This: $\\overset{\\text{chosen}}{w^{update}_{speed}}$}} \\,\\times \\left[ \\underbrace{Reward(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{\\text{Because You Do Not Know This: $r^{observed}$}}\n",
    "\\, + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_{s'}, \\text{ACTION}^{optimal})}}_{\\text{At STEP } k} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which we will thus rewrite as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\hat Q(State_s, \\overset{*random*}{\\text{ACTION}}_a)}}_{\\text{At STEP } k+1} \\longleftarrow & {} \n",
    "\\; \\left(1-\\overset{\\text{chosen}}{w^{update}_{speed}}\\right)\\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_s, \\overset{random}{\\text{ACTION}})}}_{\\text{At STEP } k} \\; + \n",
    "\\overset{\\text{chosen}}{w^{update}_{speed}} \\left[ r^{observed}\n",
    "\\, + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_{s'}, \\text{ACTION}^{optimal})}}_{\\text{At STEP } k} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\longrightarrow \\underline{\\text{If } \\overset{\\text{chosen}}{w^{update}_{speed}} \\text{ is sufficiently gradually DECREASED }}$ then this\n",
    "will converge to *true* $Q(State_s, \\text{ACTION}_a) = Q(s, a) $ for ALL $s$ and $a$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "def move(state, action):\n",
    "    \n",
    "    '''\"Randomly\" pick a new state to move to\n",
    "        according to the transition probabilites \n",
    "        and return the new state and move reward\n",
    "    '''\n",
    "    \n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    \n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked from book\n",
    "\n",
    "def exploration_policy(state):\n",
    "    \n",
    "    '''Choose between available actions \n",
    "       randomly with equal probability\n",
    "    '''\n",
    "    return np.random.choice(possible_actions[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 17.44788998,  14.56863001,  11.92009791],\n",
       "       [  0.        ,         -inf, -10.13853495],\n",
       "       [        -inf,  47.31951227,         -inf]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reworked from book\n",
    "\n",
    "# (0) initialize Q-Values to 0 for all possible actions\n",
    "\n",
    "Q_values_known_MPD = Q_values.copy()\n",
    "\n",
    "# s X a: Q_values[state, actions\n",
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  \n",
    "\n",
    "    \n",
    "# (1) initialize the tuning parameters \n",
    "\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "\n",
    "    \n",
    "# (2) Play the game!\n",
    "\n",
    "state = 0 # initial state\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # take an action\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = move(state, action)\n",
    "    \n",
    "    # get the best Q-Value of the next_state\n",
    "    next_state_current_optimal_Q = np.max(Q_values[next_state])\n",
    "    \n",
    "    # decay chosen update speed (here noted as alpha)\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    # and reweight the Q-value towards the update\n",
    "    Q_values[state, action] *= (1 - alpha)\n",
    "    Q_values[state, action] += alpha * (reward + gamma*next_state_current_optimal_Q)\n",
    "    \n",
    "    # and get ready to start again!\n",
    "    state = next_state\n",
    "    \n",
    "Q_values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values_known_MPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1809.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Approximate) Q-Learning Algorithm:  V3\n",
    "## *a Deep Q-Network (DQN) approximatng Q-Learning is called Deep Q-Learning*\n",
    "\n",
    "- off-policy algorithm\n",
    "    - Îµ-greedy policy\n",
    "    - Q-Learning using an exploration (emphasis) function\n",
    "\n",
    "### Moderate Sampling Requirements\n",
    "- catastrophic forgetting\n",
    "    - replay buffer to decorrelate actions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "# https://keras.io/api/utils/model_plotting_utils/\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from book\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    \n",
    "    '''Take the move that currently looks the best \n",
    "       with probability (1-epsilon) or uniformly \n",
    "       randomly with probability epsilon\n",
    "    '''\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from book\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    \n",
    "    '''Store Q-Value data: state and action\n",
    "       and the reward, next state, and done\n",
    "    '''\n",
    "    \n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from book\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    \n",
    "    '''Return batch of randomly \n",
    "       selected \"game instances\"\n",
    "    '''\n",
    "    \n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = \\\n",
    "    [ np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
    "    \n",
    "    return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{STARTING AT STATE } s \\text{ and following and $\\epsilon$-greedy actions }  a}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\displaystyle \n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\hat Q(State_s, \\text{ACTION}_a)}}_{\\text{At STEP } k+1} \\longleftarrow & {}\n",
    "\\underbrace{ \\sum_{s'} Pr(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{\\epsilon-\\text{greedy is mostly picking ACTION}^{optimal} }  \\,\\times \\left[ \\underbrace{Reward(State_s\\rightarrow State_{s'}| \\text{ACTION}_a)}_{{r^{observed}}}\n",
    "\\, + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_{s'}, \\text{ACTION}^{optimal})}}_{\\text{DNN estimate/prediction at STEP } k} \\right]\n",
    "\\\\\\text{i.e.} &\\\\\n",
    "\\underbrace{\\overset{\\huge \\text{UPDATE}}{\\hat Q(State_s, \\text{ACTION}^{optimal})}}_{\\text{At STEP } k+1} \\longleftarrow & {}\n",
    "\\underbrace{ 1}_{(State_s\\rightarrow State_{s'}| \\text{ACTION}^{optimal})}^{\\text{The next state in our playthrough}}  \\,\\times \\left[ \n",
    "r^{observed} + \\gamma \\cdot \\underbrace{\\overset{\\huge \\text{CURRENT}}{\\hat Q(State_{s'}, \\text{ACTION}^{optimal})}}_{\\text{DNN estimate/prediction at STEP } k} \\right]\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    \n",
    "    '''(1) Show the DNN network a batch of random game instances\n",
    "       (2) Each instance has state, action, reward, and next_state\n",
    "       (3) Predict Q(next_state, ALL_ACTIONS) and get the update target\n",
    "           Q(next_state, Action^optmal) = max Q(next_state, ALL_ACTIONS)\n",
    "    '''\n",
    "    \n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    target_Q_values = (rewards + (1-dones)*discount_factor*max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        mask = tf.one_hot(actions, n_outputs)\n",
    "        model_predicted_Q_values = model(states)\n",
    "        used_actions_model_predicted_Q_values = tf.reduce_sum(model_predicted_Q_values*mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(used_actions_model_predicted_Q_values, target_Q_values))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        \n",
    "        # decay epsilon\n",
    "        epsilon = max(1 - episode/500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if episode > 50:\n",
    "        training_step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1810.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy (a DNN for an Agent)\n",
    "    - Stochastic Policy\n",
    "        - Bayesian Bandit (exploring/exploiting)\n",
    "    - Policy Space/Search\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
