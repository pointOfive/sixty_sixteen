{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Tasks $T$: \n",
    "- Regression\n",
    "- Classification\n",
    "- The aboves, but now with missingness in inputs\n",
    "- Transcription\n",
    "    - like classification but structures in which to play classification have to be identified\n",
    "- Translation\n",
    "    - inputs are already structured, and need to be reformatted into alternative structures and vocabularies\n",
    "- Structured output\n",
    "    - outputs are multiple and tightly correlated\n",
    "    - Transcription and Translation are cases of this class\n",
    "- Anomoly detection\n",
    "    - a little bit like classification\n",
    "    - a little bit like regression\n",
    "    - a lot like distribution theory and p-values\n",
    "- Synthesis/Sampling\n",
    "    - creation of new data\n",
    "    - often structured output; but, we want lots of examples of outputs, not just a single best output\n",
    "- Imputing missing values\n",
    "    - often a regression of classification problem... a prediction problem, anyway\n",
    "    - often involves *special* considerations\n",
    "- Denoising\n",
    "    - $Pr(x|\\tilde x)$\n",
    "- Density/PMF estimation\n",
    "    - other tasks are theoretically driven by this\n",
    "    - though sometimes/often they do this implicitly with a sort of shortcut or avoid it all together so they don't actually tackle this directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures $P$: \n",
    "- Classification (with missingness) and translation/scription: accuracy/error rate (0-1 loss)\n",
    "    - scoring gets complicated when there's a lot happening, like in translation/scription\n",
    "- Regression: \"should we penalize thesystem more if it frequently makes medium-sized mistakes or if it rarely makesvery large mistakes?\" \n",
    "- distribution estimation: average log probability of examples (better models will overall assign higher log probabilities since they will more closely fit the example data)\n",
    "    - sometimes it's computationally intractible to compute this, though, so an alternative else is needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience $E$:\n",
    "\n",
    "- unsupervised: learn $p(x)$ from $x$\n",
    "    - density estimation\n",
    "        - directly\n",
    "    - implicitly:\n",
    "        - sampling/synthesis, denoising\n",
    "    - classification like with clustering\n",
    "- supervised: usually interested in $p(y|x)$\n",
    "    - label/target focussed\n",
    "    - E.g., regression, but $\\beta$'s are now called *weights*, so we write $\\hat y=w^T x + b$ (for \"bias\" term $b$ which is not included via an \"intercept\" feature in the $x$ vector)\n",
    "\n",
    "- They're not so different from a probability distributions perspective\n",
    "\n",
    "    - semi-supervised: some $y$ missing\n",
    "    - multi-instance learning: label *sets* of data points with a single $y$ label\n",
    "        - e.g., present or not in the set?\n",
    "- Reinforcement learning: adds an environment to interact with\n",
    "    - we don't cover this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacity, Under/Overfitting\n",
    "\n",
    "####  Generalization: ability to perform well on new, previously unseen data\n",
    "\n",
    "- training error versus generalization/test error\n",
    "    - generalization error: E(error on new data)\n",
    "    \n",
    "- $p_{data}$ and i.i.d. assumptions are implied\n",
    "\n",
    "#### Model fitting process\n",
    "- entails E[training error] < E[test error]\n",
    "- task is then to both\n",
    "    1. optimize Fit: E[training error] -> _underfitting is when this is poorly done_\n",
    "        - struggling to fit well\n",
    "    2. minimize Gap: E[test error]-E[training error] -> _overfitting is when this is poorly done_\n",
    "        - memorizing\n",
    "\n",
    "#### Capacity \n",
    "- means flexibility, complexity, expressiveness\n",
    "- often these are hyperparameters\n",
    "- if the model complexity -- i.e., capacity -- is appropriate for the actual complexity of the data at hand then they generally perform fairly well\n",
    "    - representational capacity and effective capacity can differ due to\n",
    "        - local optimuim\n",
    "        - numerical optimization limitations\n",
    "        - optimization algorithm imperfections, generally\n",
    "    - regularization\n",
    "        - penalty term to favor [_whatever_]\n",
    "        - attempts/intends to reduce generalization error but not training error\n",
    "        - only optimization is an equally influential aspect of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation\n",
    "\n",
    "- $Bias(\\hat \\theta_m) = E[\\hat \\theta_m - \\theta]$, assymptotically unbiased if this approaches 0 as $m$ grows\n",
    "\\begin{align*}\n",
    "E\\left[\\frac{\\sum_{i=1}^m (x_i - \\bar x)^2}{m} \\right] &=\n",
    "E\\left[\\frac{\\sum_{i=1}^m ((x_i - \\mu) - (\\bar x - \\mu))^2}{m} \\right] \\\\\n",
    "&= E\\left[\\frac{\\sum_{i=1}^m \\left( (x_i - \\mu)^2 - 2(x_i - \\mu)(\\bar x - \\mu) + (\\bar x - \\mu)^2 \\right) }{m} \\right] \\\\\n",
    "&= E\\left[\\frac{\\sum_{i=1}^m (x_i - \\mu)^2 - 2m(\\bar x - \\mu)(\\bar x - \\mu) + m(\\bar x - \\mu)^2}{m} \\right] \\\\\n",
    "&= E\\left[\\frac{\\sum_{i=1}^m (x_i - \\mu)^2 - m(\\bar x - \\mu)^2}{m} \\right] \\\\\n",
    "&= \\frac{\\sum_{i=1}^m E[(x_i - \\mu)^2] - m E[(\\bar x - \\mu)^2]}{m} \\\\\n",
    "&=  E[(x_i - \\mu)^2] - E[(\\bar x - \\mu)^2] \\\\\n",
    "&= \\sigma^2 - \\frac{\\sigma^2}{m} \\\\\n",
    "&= \\frac{m-1}{m} \\sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "So multiply everything through by $\\frac{m}{m-1}$ to get the unbiased estimate: \n",
    "- you can see the denomenator is $m-1$.\n",
    "\n",
    "#### MSE\n",
    "\n",
    "\\begin{align*}\n",
    "MSE &= E_{\\hat \\theta}\\left[(\\hat \\theta - \\theta)^2\\right]\\\\\n",
    "&= E_{\\hat \\theta}\\left[\\left((\\hat \\theta - E[\\hat \\theta]) + (E[\\hat \\theta] - \\theta)\\right)^2\\right]\\\\\n",
    "&= E_{\\hat \\theta}\\left[(\\hat \\theta - E[\\hat \\theta])^2\\right] + 2 E_{\\hat \\theta}\\left[\\hat \\theta - E[\\hat \\theta]\\right](E[\\hat \\theta]-\\theta) + (E[\\hat \\theta]-\\theta)^2\\\\\n",
    "&= E_{\\hat \\theta}\\left[(\\hat \\theta - E[\\hat \\theta])^2\\right] + 0 + (E[\\hat \\theta]-\\theta)^2\\\\\n",
    "&= Var[\\hat \\theta ] + Bias\\left(E[\\hat \\theta]\\right)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### (Weak) Consistency\n",
    "\n",
    "- For any $\\epsilon>0, Pr(|\\hat \\theta_m − \\theta| > \\epsilon) \\rightarrow 0$ as $m \\rightarrow \\infty$\n",
    "    - Almost surely (strong consistency) if $Pr\\left(\\underset{m \\rightarrow \\infty}{lim} \\hat \\theta_{m} = \\theta\\right) = 1$\n",
    "- both mean bias is decreasing with more data $m$\n",
    "- but just because an estimator is unbiased doesn't mean it is getting closer and closer to the true parameter\n",
    "    - consistency means it's variance around the true estimator is incessantly decreasing with increasing data $m$. \n",
    "    \n",
    "#### MLE\n",
    "\n",
    "- best estimator assymptotically (in terms of convergence rate) as $m$ increases\n",
    "    - has (weak) consistency if (a) $p_{data}$ is in the $p_{model}$ model family and (b) $\\hat p_{data}$ is indeed generated from only one such member of the $p_{model}$ model family\n",
    "    - no other consistent estimator is more efficient (smaller MSE) for large $m$ (by the  Cramér-Rao lower bound)\n",
    "- $p_{data}(\\mathbf{x})$ and it's approximate model $\\prod p_{model}(\\mathbf{x},\\theta)$\n",
    "- $\\underset{\\theta}{argmax} \\prod_i p_{model}(\\mathbf{x_i},\\theta) = \\underset{\\theta}{argmax} \\sum_i \\log p_{model}(\\mathbf{x_i},\\theta)$ (for underflow avoidance purposes)\n",
    "- minimizes $KL[\\hat p_{data}||\\log p_{model}] = E_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{data}(\\mathbf{x}) - \\log p_{model}(\\mathbf{x},\\theta)]$\n",
    "    - i.e., minimizes $- E_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{model}(\\mathbf{x},\\theta)]$, i.e., maximizes $E_{\\mathbf{x} \\sim \\hat p_{data}}[\\log p_{model}(\\mathbf{x},\\theta)]$\n",
    "\n",
    "        \n",
    "- _cross-entropy_: any negative log likelihood loss\n",
    "    - but usually used to refer to negative loglikelihood of a Bernoulli or softmax distribution\n",
    "    - nonetheless, MSE *IS* the cross-entropy between the empirical distribution and a Gaussian model, i.e., the negative log likelihood of a Gaussian model\n",
    "    \n",
    "   \n",
    "\n",
    "#### Bayes \n",
    "\n",
    "- averages over uncertainty... so doesn't chase optimals... and instead chases generalizability\n",
    "- adds a prior\n",
    "    - can regularize\n",
    "    - can add in information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Bayesian estimation tends to generalize better than MLE because\n",
    "1. it averages over all possible models based on their uncertainty\n",
    "2. MLE estimates tend to suffer from statistical efficiency problems\n",
    "3. consistency is more challenging in MLE relative to Bayesian analysis\n",
    "4. a prior provides regularization which helps improve generalization\n",
    "\n",
    "Which of the following are not regularization specifications\n",
    "1. $\\underset{\\theta}{argmax} \\log p(\\theta|\\mathbf{x}) + \\log p(\\theta)$\n",
    "2. $\\underset{w}{argmax} \\sum_i(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 + \\lambda \\mathbf{w}^T\\mathbf{w}$\n",
    "3. $\\underset{w}{argmax} ||\\mathbf{X}\\mathbf{w}-\\mathbf{y}||^2_2 + \\lambda ||\\mathbf{w}||^2_2$\n",
    "4. $\\hat y = \\mathbf{w}^T\\mathbf{x}_i + b$ \n",
    "\n",
    "A \"broad\", \"wide\", or \"uninformative\" prior\n",
    "1. has high entropy\n",
    "2. has low entropy\n",
    "3. expresses no preference about parameters\n",
    "\n",
    "The most appropriate definition of _cross-entropy_:\n",
    "\n",
    "1. any loss involving the negative log likelihood of the empirical data distribution\n",
    "    - i.e., the cross-entropy between the model and empirical distribution\n",
    "2. negative log-likelihood of a Bernoulli or softmax distribution\n",
    "3. the Kullback-Leibler divergence $KL[p_{data}|| p_{model}]$\n",
    "4. Maximum likelihood estimation $\\underset{\\theta}{argmax} \\sum_i \\log p_{model}(\\mathbf{x_i},\\theta)$ \n",
    "\n",
    "\n",
    "Both $- \\sum_i  \\log p_{model}(\\mathbf{x_i},\\theta)$ and $KL[\\hat p_{data}|| p_{model}]$ ***are equivalently minimized***, and have bounds\n",
    "\n",
    "1. $(-\\infty,\\infty)$\n",
    "2. $(-\\infty,\\infty)$ and $[0,\\infty)$\n",
    "3. $(-\\infty,\\infty)$ and $(-\\infty,0]$\n",
    "4. $[0,\\infty)$ and $(-\\infty,0]$\n",
    "5. $(-\\infty,0]$\n",
    "6. $[0,\\infty)$\n",
    "\n",
    "\n",
    "$KL[\\hat p_{data}|| p_{model}], \\; \\prod_i p_{model}(\\mathbf{x_i},\\theta), \\;$ and _cross-entropy_\n",
    "\n",
    "1. address different classes of problems\n",
    "2. refer to the same objective function \n",
    "3. have equivalent optimization solutions\n",
    "\n",
    "\n",
    "\n",
    "The estimation in Bayesian statistics is\n",
    "1. a probability distribution\n",
    "2. a point estimate\n",
    "3. better than an MLE estimate\n",
    "    - not according to the Cramér-Rao lower bound\n",
    "    - not according to computational tractability (in many \"large $m$\" contexts)\n",
    "    - but *does* generally perform better generalization\n",
    "        - because Bayesian estimation incorporates uncertainty directly into it's calculation...\n",
    "        - so if it doesn't know something well it doesn't act like it does because it's the best choice over other choices        \n",
    "4. more subjective than MLE estimation\n",
    "    - some say the prior is subjective\n",
    "    - but so is the choice of likelihood\n",
    "    - and hence the choice of your Maximum Likelihood estimator\n",
    "5. has an expected value that must be biased if the alternative MLE estimator is unbiased\n",
    "    - no: an improper prior could be used\n",
    "\n",
    "Maximum Likelihood Estimation \n",
    "\n",
    "1. does not lend itself to cost function minimization\n",
    "2. uses neither an objective function nor a cost function\n",
    "3. is formal statistical methodology with extremely close ties to ML\n",
    "4. is _never_ used by any self respecting \"Bayesian\" statistician\n",
    "\n",
    "Bayesian analysis does not\n",
    "1. allow information outside of the data to be included into the analysis\n",
    "2. provide a mechanism to construct parameter regularization strategies \n",
    "3. attempt to remove bias from the parameter estimation process, generally\n",
    "4. use probability as a language to make belief statements on parameters\n",
    "\n",
    "Which concluding statement is false?  Logistic and Linear Regression\n",
    "1. are both based on probability models\n",
    "2. can both be optimized analytically\n",
    "3. make their predictions using linear models\n",
    "4. predict different kinds of response varaibles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The *support vector machine* (SVM)\n",
    "1. has the form $\\mathbf{w}^T\\mathbf{x}+b$\n",
    "2. which predicts based on $sign(\\mathbf{w}^T\\mathbf{x}+b)$\n",
    "3. where $\\mathbf{w}=\\sum_i \\alpha_i \\mathbf{x}_i$\n",
    "4. so $\\mathbf{w}^T\\mathbf{x}+b = b + \\alpha_i \\mathbf{x}^T\\mathbf{x}_i$\n",
    "5. or, $b + \\alpha_i \\phi(\\mathbf{x})\\cdot\\phi(\\mathbf{x}_i)$ for any space transform $\\phi$ and inner product therein\n",
    "  1. e.g., vector inner product $\\phi(\\mathbf{x})^T\\phi(\\mathbf{x}_i)$\n",
    "6. or, $b + \\alpha_i \\kappa(\\mathbf{x}, \\mathbf{x}_i)$ for any kernel $\\kappa$ which calculates the inner product under the $\\phi$ transform\n",
    "  1. $\\phi$ could be an infinite dimensional space...\n",
    " \n",
    "- the *kernel trick* \n",
    "    - provides highly non-linear models of x\n",
    "        - (which *are*, of course, linear in the $\\phi$ space)\n",
    "        - (but the $\\phi$ space can be a highly non-linear transform of $x$)\n",
    "    - is often extremely computationally efficient, and optimization for $\\boldsymbol{\\alpha}$ turns out to be a quite straightforward convex optimization problem, as, given the kernel, the model is linear in the $\\boldsymbol{\\alpha}$\n",
    "    \n",
    "    \n",
    "- Radial Basis Function (RBF) \n",
    "    - $\\kappa(\\mathbf{u},\\mathbf{v}) = N(\\mathbf{u}-\\mathbf{v};\\mathbf{0}, \\sigma^2I)$\n",
    "    - functions like template matching \n",
    "        - the better the match the more weight the matches training $y$ value is given\n",
    "\n",
    "- non-zero $\\alpha_i$ are called support vectors.  If most are zero the evaluation of $b + \\alpha_i \\mathbf{x}^T\\mathbf{x}_i$ is cheaper\n",
    "- computational cost of optimization is also quite steep for SVMs...\n",
    "- kernel methods/machines use the kernel trick\n",
    "    - but generic kernels seem to have trouble generalizing super well\n",
    "\n",
    "#### others\n",
    "- KNN: cannot distinguish when one feature is more important\n",
    "    - at most $m$ \"smoothness\" areas\n",
    "- DT: struggles for x_1>x_2 means class 1\n",
    "    - at most $m$ \"smoothness\" areas\n",
    "\n",
    "\n",
    "### Unsupervised -- no labeling effort\n",
    "- density estimation\n",
    "- learning to sample from a distribution\n",
    "- learning to denoise\n",
    "- finding manifolds where data resides\n",
    "- grouping data into clusters\n",
    "  \n",
    "- \"find 'best' representation of data\"\n",
    "    - lower dimensional representation\n",
    "    - sparse representation\n",
    "    - indepdendent representations (dis-entangle contributing factors)\n",
    "    \n",
    "- PCA\n",
    "    - lower dimensional representation and uncorrelated (so, *partially* independent)\n",
    "- K-means\n",
    "    - sparse one-hot-encoding representation and dimensionality reduction\n",
    "    - what level of grouping hierarchy is appropriate?\n",
    "\n",
    "## Deep Learning\n",
    "1. provides better generalization based on medium-sized data sets\n",
    "2. provdes a scalable approach to nonlinear models on large data sets\n",
    "\n",
    "VIA:\n",
    "\n",
    "### SGD (Stochastic Gradient Descent)\n",
    "- GD traditional regarding as foolhardy or unprincipled for non-convex optimization\n",
    "- but it works really quite fine for ML models\n",
    "- even when it doesn't even arrive at a local minimum it's usually pretty good at arriving a quite fine level of the cost functio fairly quickly\n",
    "\n",
    "\n",
    "#### ML Algorithms\n",
    "1. data\n",
    "2. model\n",
    "3. cost function\n",
    "4. optimization algorithm\n",
    "\n",
    "- smoothness prior *is not enough*!\n",
    "- need to be able to generalize nonlocally...\n",
    "    - composition of factors is one such assumption/approach\n",
    "    - they provide exponential gains that can counteract the curse of dimensionality\n",
    "    - manifold learning algorithms also reduce dimensions under consideration\n",
    "        1. concentrated probability densities are a prerequisite\n",
    "        2. and if local transformations seem plausible then maybe so!\n",
    "        - representing in manifold coordinates facilitates the use of ML tools on manifolds\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
