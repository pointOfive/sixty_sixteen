{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "# https://stackoverflow.com/questions/57658935/save-jupyter-notebook-with-plotly-express-widgets-displaying\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- We'll use some Keras data functionality this time\n",
    "    - Again we get the nice local data management -- no need for repeated downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 365s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51235508/in-which-folder-on-pc-windows-10-does-load-data-save-a-dataset-in-keras\n",
    "! ls ~/.keras/datasets/cifar-10-batches-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at our ~~images~~ ~~data~~ tensors\n",
    "- Keras loads all the data into memory: *not* a TFDS\n",
    "    - In fact, not even tensors: they're `numpy.ndarray`'s\n",
    "  \n",
    "  \n",
    "- *Still, Images are 'Tensors', in the 'neaural network' sense (though not in the physics/linear algebra sense)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `...` provides convenient indexing expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0 \n",
    "train_images[image_index].shape, train_images[image_index,...].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `np.newaxis` provides convenient dimensionality expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/17394882/how-can-i-add-new-dimensions-to-a-numpy-array\n",
    "train_images[np.newaxis,image_index,...].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humans See This\n",
    "(using code borrowed from the [TF CNN tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb#scrollTo=K3PAELE2eSU9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb#scrollTo=K3PAELE2eSU9\n",
    "def plot_cifar10(x,y):\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    plt.subplot(111);plt.xticks([]);plt.yticks([]);plt.grid(False)\n",
    "    plt.imshow(x, cmap=plt.cm.binary);plt.xlabel(class_names[y[0]])\n",
    "    \n",
    "plot_cifar10(train_images[image_index],train_labels[image_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computers See This\n",
    "- note that the scale of the data is 0-255 *and likely needs to be changed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images[image_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = {v:i for i,v in enumerate(\"rgb\")}\n",
    "print(rgb)\n",
    "\n",
    "image_index = 0 \n",
    "train_images[image_index][..., rgb['r']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/3d-scatter-plots/\n",
    "# https://plotly.com/python/colorscales/\n",
    "# https://plotly.com/python/creating-and-updating-figures/\n",
    "# https://community.plotly.com/t/plotly-express-multiple-plots-overlay/31984\n",
    "\n",
    "def plot_channel(x, h, color, alpha, loc, i=[0], j=[0]):\n",
    "\n",
    "    i_grid,j_grid = np.meshgrid(*[range(i) for i in x[0].shape[:2]])\n",
    "\n",
    "    df = pd.DataFrame(columns=['i','j','h','c'])\n",
    "    for x_,h_,i_,j_ in zip(x,h,i,j):\n",
    "        tmp = pd.DataFrame({'i': i_+i_grid.ravel(), 'j': j_+j_grid.ravel(), 'h': h_+0*i_grid.ravel()})\n",
    "        tmp['c'] = x_.ravel()\n",
    "        df = df.append(tmp)\n",
    "    \n",
    "    fig.append_trace(go.Scatter3d(x=df.i, y=df.j, z=df.h, opacity=alpha, \n",
    "                           mode='markers', marker={'color':df.c, 'colorscale': color}),*loc)\n",
    "\n",
    "# a few other helpful posts which ultimately led to my solution above\n",
    "# https://community.plotly.com/t/specifying-a-color-for-each-point-in-a-3d-scatter-plot/12652\n",
    "# https://www.reddit.com/r/rstats/comments/g3tulu/is_it_possible_to_vary_alphaopacity_by_group_in_a/\n",
    "# https://plotly.com/python/3d-scatter-plots/\n",
    "# .. mode='markers', marker=dict(color='(255,0,0)', size=10) ...\n",
    "# https://stackoverflow.com/questions/53875880/convert-a-pandas-dataframe-of-rgb-colors-to-hex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46750462/subplot-with-plotly-with-multiple-traces\n",
    "fig = plotly.subplots.make_subplots(rows=1,cols=2, specs=[[{'type': 'scene'}, {'type': 'scene'}]])\n",
    "\n",
    "plot_channel([train_images[image_index][..., rgb['r']]/255], [0], 'Reds', 0.33, [1,1])\n",
    "plot_channel([train_images[image_index][..., rgb['g']]/255], [1], 'Greens', 0.33, [1,1])\n",
    "plot_channel([train_images[image_index][..., rgb['b']]/255], [2], 'Blues', 0.33, [1,1])\n",
    "\n",
    "plot_channel([train_images[image_index][..., rgb['r']]/255,\n",
    "              train_images[image_index][..., rgb['g']]/255,\n",
    "              train_images[image_index][..., rgb['b']]/255], [0,1,2], 'Greys', 0.33, [1,2], [0]*3, [0]*3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what are \"Convolutional\" Kernels?\n",
    "- \"Convolutional\" is in quotes because \n",
    "    - the operation these *kernels* perform is actually a \"cross-correlation\" (i.e., similarity measure)\n",
    "        - *inerestingly... kernel is also a highly overloaded term...\"* but in the CNN context mean the [matrix](https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216) doing \"cross-correlations\" over the data\n",
    "    - but the idea of a convolution is to move a function across another, which is reasonable as an intution for CNNs\n",
    "        - [Mathematical Convolution](https://lpsa.swarthmore.edu/Convolution/CI.html)\n",
    "        - [CNN Kernel \"Convolution\"](https://ezyang.github.io/convolution-visualizer/index.html)\n",
    "        \n",
    "        \n",
    "**Not an official textbook; but, [Dive Into Deep Learning](http://d2l.ai/chapter_convolutional-neural-networks/channels.html) is another free resource that I've found useful for finding quick answers to things**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the terms kernel/filter as recommended on: https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216\n",
    "number_filters = 10\n",
    "number_channels = 3\n",
    "kernel_width = 5\n",
    "\n",
    "kernels = np.random.rand(number_filters*number_channels*kernel_width*kernel_width)\n",
    "kernels = kernels.reshape((kernel_width,kernel_width,number_channels,number_filters))-0.5\n",
    "print(kernels.shape)\n",
    "\n",
    "# using the terms kernel/filter as recommended on: https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216\n",
    "filter_index = 0\n",
    "channel_index = rgb['r']\n",
    "kernels[...,color_channel,filter_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so God commanded..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0\n",
    "i,j = 0,0\n",
    "\n",
    "for channel in 'rgb':\n",
    "    channel_index = rgb[channel]\n",
    "    print(channel)\n",
    "    print(train_images[image_index,\n",
    "                       i:(i+kernel_width),\n",
    "                       j:(j+kernel_width), \n",
    "                       channel_index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet Kernel\n",
    "(for each color channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_index = 0\n",
    "\n",
    "for channel in 'rgb':\n",
    "    channel_index = rgb[channel]\n",
    "    print(channel)\n",
    "    print(kernels[:,:,channel_index,filter_index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Multiply (and Sum (across color chanels, too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_index = 0\n",
    "image_index = 0\n",
    "i,j = 0,0#1,1\n",
    "\n",
    "(\\\n",
    "train_images[image_index, \n",
    "             i:(i+kernel_width), \n",
    "             j:(j+kernel_width), \n",
    "             :] * kernels[...,filter_index]\\\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what's this doing to the big picture as a TF/Keras layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape, kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will result in\n",
    "50000,32-5+1,32-5+1,10\n",
    "# assuming no padding, stride, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transforms each image into a \"10 channel\" 28 by 28 image\n",
    "- Each \"channel\" captures \"hotness\" of a certain \"feature\"\n",
    "    - captured via cross-correlation with the kernel characterizing the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So \"Kernels\" are features (you say)? How so (pray tell)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- E.g., horizontal edge detection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels[:2,...,-1]=-1\n",
    "kernels[2,...,-1]=0\n",
    "kernels[3:,...,-1]=1\n",
    "kernels[...,0,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- or vertical edge detection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels[:,:2,:,-2]=-1\n",
    "kernels[:,2,:,-2]=0\n",
    "kernels[:,3:,:,-2]=1\n",
    "kernels[...,0,-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which gives us this sort of thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional_layer_output = np.zeros(list(np.array(train_images[image_index].shape[:2])-kernel_width+1)+[number_filters])\n",
    "print(convolutional_layer_output.shape)\n",
    "\n",
    "for filter_index in range(number_filters):\n",
    "    for i in range(convolutional_layer_output.shape[0]):\n",
    "        for j in range(convolutional_layer_output.shape[1]):\n",
    "            convolutional_layer_output[i,j,filter_index] = \\\n",
    "            (kernels[...,filter_index] *\n",
    "             (train_images[image_index,\n",
    "                          i:(i+kernel_width),\n",
    "                          j:(j+kernel_width), \n",
    "                          :]/255-0.5)).sum() # again, notice that this sums over color channels;\n",
    "                                             # although, each color channel gets its own kernel.\n",
    "                                             # ALSO NOTE the standardizing: but this only changes the output scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i.e., completing the full \"convolution\" over the whole image, gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "ax1.imshow(convolutional_layer_output[...,-1])\n",
    "ax2.imshow(train_images[image_index], cmap=plt.cm.binary)\n",
    "ax3.imshow(convolutional_layer_output[...,-2])\n",
    "ax4.imshow(kernels[...,0,-1])\n",
    "ax5.axis('off')\n",
    "ax6.imshow(kernels[...,0,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical edge detection kernel\n",
    "- detects \"left-to-right\" \"low-to-high\" as positive\n",
    "- detects \"left-to-right\" \"high-to-low\" as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[0],[1],[0]]).dot(np.array([[0,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might try to animate this later:\n",
    "# https://plotly.com/python/animations/\n",
    "# https://plotly.com/python/v3/gapminder-example/\n",
    "\n",
    "fig = plotly.subplots.make_subplots(rows=1,cols=2, specs=[[{'type': 'scene'}, {'type': 'scene'}]])\n",
    "\n",
    "plot_channel([train_images[image_index][..., rgb['r']]/255,\n",
    "              train_images[image_index][..., rgb['g']]/255,\n",
    "              train_images[image_index][..., rgb['b']]/255], [0,1,2], 'Greys', 0.33, [1,1], [0]*3, [0]*3)\n",
    "\n",
    "for filter_index in range(number_filters):\n",
    "    plot_channel([convolutional_layer_output[...,filter_index]], [3*filter_index], \n",
    "                 px.colors.named_colorscales()[filter_index], 0.1, [1,2])\n",
    "\n",
    "\n",
    "filter_index=5\n",
    "i,j=17,17\n",
    "plot_channel(np.moveaxis(kernels[...,filter_index],-1,0), list(range(3)), \n",
    "             px.colors.named_colorscales()[filter_index], 1, [1,1], i=[i]*3, j=[j]*3)\n",
    "\n",
    "plot_channel([np.array([[0],[1],[0]]).dot(np.array([[0,1,0]]))], [3*filter_index], \n",
    "             px.colors.named_colorscales()[filter_index], 1, [1,2], i=[i-1], j=[j-1])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary/Review\n",
    "\n",
    "- We make multiple distinct new \"feature layers\" (i.e., channels) *outputs*\n",
    "- Kernels for each *input channel* are [different for each *input channel*](https://stackoverflow.com/questions/43306323/keras-conv2d-and-input-channels)\n",
    "- *A bias (intercept) term (ONE for EACH filter) is usually included after all the multiplying and summing*\n",
    "    \n",
    "    - `#BIAS_TERM = number_filters*[0]`\n",
    "    \n",
    "    - `# + BIAS_TERM[filter_index]`\n",
    "\n",
    "\n",
    "**What does it mean when we start chain these things?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is *LEARNING* these *KERNELS* hard for NNs... gradients and such?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. It's just a set of linear transformations; so\n",
    "2. It can be represented as a matrix multiplication; this also means\n",
    "3. The partial derivatives (and hence the gradient) of the weights is easy to calculate\n",
    "4. And can again be represented as a ([transposed](https://stats.stackexchange.com/questions/335332/why-use-matrix-transpose-in-gradient-descent)) matrix multipliction\n",
    "    - i.e., encoded into the tensor graph and quickly computed by TF\n",
    "\n",
    "$$\\huge \n",
    "\\begin{align*}\n",
    "Y'_{ij} = {}& g_{ijk}(X)\\\\\n",
    "= {}& \\sum_{c}\\sum_{i_0 = 0:kw}\\sum_{j_0 = 0:kw} X_{(i+i_0)(j+j_0)c} K_{i_0j_0k_0}\\\\\n",
    "Y_{ij} = {}& f(g_{ijk}(X)) \\\\\n",
    "\\frac{\\partial}{\\partial K_{abk}} Y_{ij} = {} & \\frac{Y_{ij}}{\\partial Y'_{ij}} \\frac{\\partial Y'_{ij}}{\\partial K_{abk}}\\\\\n",
    "= {}& \\frac{Y_{ij}}{\\partial Y'_{ij}} \\sum_{c} X_{(i+a)(j+b)c} \\\\\n",
    "\\frac{\\partial}{\\partial K_{abk}} \\sum_{i,j} Y_{ij} = {} & \\sum_{i,j} \\frac{Y_{ij}}{\\partial Y'_{ij}} \\frac{\\partial Y'_{ij}}{\\partial K_{abk}}\\\\\n",
    "= {}& \\sum_{i,j} \\frac{Y_{ij}}{\\partial Y'_{ij}} \\sum_{c} X_{(i+a)(j+b)c} \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Max) Pooling\n",
    "- Creates local translation invariance\n",
    "    - which is good when absolute locations are less important than approximate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_width = 4\n",
    "\n",
    "pooling_layer_result = np.zeros(list((np.array(convolutional_layer_output.shape[:2])/pooling_width).astype(int))+[convolutional_layer_output.shape[2]])\n",
    "print(pooling_layer_result.shape)\n",
    "\n",
    "i_pooling_layer_result = 0*pooling_layer_result\n",
    "j_pooling_layer_result = 0*pooling_layer_result\n",
    "\n",
    "for k in range(number_filters):\n",
    "    for ii,i in enumerate(range(0, convolutional_layer_output.shape[1], pooling_width)):\n",
    "        for jj,j in enumerate(range(0, convolutional_layer_output.shape[1], pooling_width)):\n",
    "            pooling_layer_result[ii,jj,k] = convolutional_layer_output[i:(i+pooling_width),j:(j+pooling_width),k].max()\n",
    "            i_pooling_layer_result[ii,jj,k] = ii\n",
    "            j_pooling_layer_result[ii,jj,k] = jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_index = 8\n",
    "\n",
    "tmp = [convolutional_layer_output[i:(i+pooling_width),j:(j+pooling_width),filter_index]/255\n",
    "       for j in range(0,28,8) for i in range(4,28,8)]\n",
    "tmp_i = [i for j in range(0,28,8) for i in range(4,28,8)]\n",
    "tmp_j = [j for j in range(0,28,8) for i in range(4,28,8)]\n",
    "tmp += [convolutional_layer_output[i:(i+pooling_width),j:(j+pooling_width),filter_index]/255\n",
    "        for i in range(0,28,8) for j in range(4,28,8)]\n",
    "tmp_i += [i for i in range(0,28,8) for j in range(4,28,8)]\n",
    "tmp_j += [j for i in range(0,28,8) for j in range(4,28,8)]\n",
    "\n",
    "\n",
    "jmp = [pooling_layer_result[i:(i+1),j:(j+1),filter_index]/255\n",
    "       for j in range(0,7,2) for i in range(1,7,2)]\n",
    "jmp_i = [i for j in range(0,7,2) for i in range(1,7,2)]\n",
    "jmp_j = [j for j in range(0,7,2) for i in range(1,7,2)]\n",
    "jmp += [pooling_layer_result[i:(i+1),j:(j+1),filter_index]/255\n",
    "        for i in range(0,7,2) for j in range(1,7,2)]\n",
    "jmp_i += [i for i in range(0,7,2) for j in range(1,7,2)]\n",
    "jmp_j += [j for i in range(0,7,2) for j in range(1,7,2)]\n",
    "\n",
    "kern_indx = 2\n",
    "\n",
    "tmp = [tmp]\n",
    "tmp_i = [tmp_i]\n",
    "tmp_j = [tmp_j]\n",
    "\n",
    "tmp.append([convolutional_layer_output[i:(i+pooling_width),j:(j+pooling_width),filter_index]/255\n",
    "       for j in range(0,28,8) for i in range(4,28,8)])\n",
    "tmp_i.append([i for j in range(0,28,8) for i in range(4,28,8)])\n",
    "tmp_j.append([j for j in range(0,28,8) for i in range(4,28,8)])\n",
    "tmp[-1] += [convolutional_layer_output[i:(i+pooling_width),j:(j+pooling_width),filter_index]/255\n",
    "        for i in range(0,28,8) for j in range(4,28,8)]\n",
    "tmp_i[-1] += [i for i in range(0,28,8) for j in range(4,28,8)]\n",
    "tmp_j[-1] += [j for i in range(0,28,8) for j in range(4,28,8)]\n",
    "\n",
    "jmp = [jmp]\n",
    "jmp_i = [jmp_i]\n",
    "jmp_j = [jmp_j]\n",
    "\n",
    "jmp.append([pooling_layer_result[i:(i+1),j:(j+1),filter_index]/255\n",
    "       for j in range(0,7,2) for i in range(1,7,2)])\n",
    "jmp_i.append([i for j in range(0,7,2) for i in range(1,7,2)])\n",
    "jmp_j.append([j for j in range(0,7,2) for i in range(1,7,2)])\n",
    "jmp[-1] += [pooling_layer_result[i:(i+1),j:(j+1),filter_index]/255\n",
    "        for i in range(0,7,2) for j in range(1,7,2)]\n",
    "jmp_i[-1] += [i for i in range(0,7,2) for j in range(1,7,2)]\n",
    "jmp_j[-1] += [j for i in range(0,7,2) for j in range(1,7,2)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Max) Pooling\n",
    "- Creates local translation invariance\n",
    "    - which is good when absolute locations are less important than approximate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotly.subplots.make_subplots(rows=1,cols=2, specs=[[{'type': 'scene'}, {'type': 'scene'}]])\n",
    "\n",
    "for filter_index in range(number_filters):\n",
    "    plot_channel([convolutional_layer_output[...,filter_index]/255], [3*filter_index], \n",
    "                 'Greys', 0.1, [1,1])\n",
    "\n",
    "for filter_index in range(number_filters):\n",
    "    plot_channel([pooling_layer_result[...,filter_index]/255], [3*filter_index], \n",
    "                 'Greys', 0.2, [1,2])\n",
    "\n",
    "\n",
    "for k,filter_index in enumerate([8,2]):\n",
    "    plot_channel(tmp[k], [3*filter_index]*len(tmp[k]), px.colors.named_colorscales()[filter_index], \n",
    "                 1, [1,1], i=tmp_i[k], j=tmp_j[k])    \n",
    "    plot_channel(jmp[k], [3*filter_index]*len(jmp[k]), px.colors.named_colorscales()[filter_index], \n",
    "                 1, [1,2], i=jmp_i[k], j=jmp_j[k])    \n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we used more filters, and [\"max pooled\" across filters](https://stackoverflow.com/questions/36817868/tensorflow-how-to-pool-over-depth/36853403) we could create \"kernel invariance\"\n",
    "\n",
    "![kernel invariance](https://www.programmersought.com/images/568/1d315847250c7ae6ecfddd8928f2ca90.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Lecture\n",
    "- [TensorFlow CNN Tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb) achieves ~70% accuracy for CIFAR-10\n",
    "    - See also the [TensorFlow CNN Tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb); althogh, it's not for CIFAR-10\n",
    "- How might we improve this?\n",
    "    - Increase the network size?\n",
    "        - Bigger?  Wider?  Deeper?\n",
    "            - `padding='same'`?\n",
    "    - Regularize the network?\n",
    "        - Dropout?\n",
    "        - Kernel Shrinkage?\n",
    "        - Batch Normalization?\n",
    "        - Data Augmentation?\n",
    "        - Noise? \n",
    "- We can get up to nearly 90% accuracy [like this](https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/)\n",
    "    - That's [\"state of the art\" for 2015](https://benchmarks.ai/cifar-10)\n",
    "    - What's the epochs specification?\n",
    "    - We've changed the application order of BatchNormalizatoin/ReLU steps following the advice of the book, but it seems [BatchNormalizatoin application order is still be an open question](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)\n",
    "    - We've changed `elo` to `relo` but this [might not be the best choice](https://www.reddit.com/r/MachineLearning/comments/6g15si/d_elu_vs_relu_any_new_benchmarks/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mean = np.mean(train_images,axis=(0,1,2,3))\n",
    "std = np.std(train_images,axis=(0,1,2,3))\n",
    "train_images = (train_images-mean)/(std+1e-7)\n",
    "test_images = (test_images-mean)/(std+1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "# https://keras.io/api/preprocessing/image/\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rotation_range=15,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1: Initial Convolution\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3),\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "#model.add(Activation('elu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 2: Convolution on Initial Convolution followed by Pooling with Dropout\n",
    "model.add(Conv2D(32, (3,3), padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "# Layer 3: Plain Convolution again\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "s\n",
    "# Layer 4: Convolution on Convolution followed by Pooling with Dropout, again\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "# Layer 5: another Plain Convolution\n",
    "model.add(Conv2D(128, (3,3), padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 6: another Convolution-Pooling-Dropout layer\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "# Layer 7: final\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(10))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.0005\n",
    "    if epoch > 75:\n",
    "        lrate = 0.00025\n",
    "    if epoch > 125:\n",
    "        lrate = 0.00001\n",
    "    return lrate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6),#'adam', \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),#tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(datagen.flow(train_images,  train_labels, batch_size=64), \n",
    "                    epochs=150, validation_data=(test_images, test_labels),\n",
    "                    callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARSE REPRESENTATION WON'T WORK\n",
    "# WE NEED TO CONVERT TO CATEGORICAL\n",
    "# https://keras.io/api/preprocessing/image/\n",
    "# https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "from keras.utils import np_utils\n",
    "train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "test_labels = np_utils.to_categorical(test_labels, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After reading ch 9 you should be aware of\n",
    "\n",
    "\n",
    "## the key details and specifics of CNNs\n",
    "\n",
    "- each convolutional kernel extracts a specific feature\n",
    "- by CNN we actually mean the application of multiple parallel different kernels\n",
    "- *striding* is often used to downsample the created features, which can of course be computationally benefical\n",
    "- *zero-padding* can keep the network from shrinking with each layer\n",
    "    - *valid* convolutions don't zero pad so each output is based on the same number of inputs and they all behave \"regularly\" in this sense\n",
    "    - *same* convolutions keep the output shape the same as the input shape, so the network can be arbitrarily deep, but also, edge/border inputs are mapped to fewer nodes in the next layer and so are less represented in the model\n",
    "    - *full* \"overpads\" from the get go to make sure each input is equitably passed into the next layer in terms of it's number of connections, but this means at the next layer the edge units are based on different numbers of input units... \n",
    "        - the last point causes the third option to not usually pereform particularly well, so somewhere between the first two options is generally preferred...\n",
    "        \n",
    "- biases are typically attached according to the natural structures of the convolutional archetecture, but they could also be set to be unique at each locality, for example. \n",
    "\n",
    "    - *unshared* convolutions are a way to parameterize local structure feature extraction, i.e., create location specific features\n",
    "    - *tiled* convolutions are another variant where parameters governing connections are selectively specified according to some predefined patterns\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "## what the Benefits of CNNs are\n",
    "\n",
    "- *sparse interactions* (*sparse connectivity* or *sparse weights*): when inputs to the next hidden layer are defined by a kernel that is smaller than the inputs \n",
    "- *parameter sharing* (*tied weights*): when the weights applied at each application of the kernel are the same (i.e. repeatedly re-used) during each application of the kernel (i.e., as the kernel is \"swept\" over the input) \n",
    "- *equivariant representations*:\n",
    "    - convolution $f$ is *equivariant* to translation $g$ since (for image $x$) transitioning the convolution $g(f(x))$ is the same as the convolution of the trasition $f(g(x))$: $g(f(x)) = f(g(x))$ \n",
    "    - convolution is not *equivariant* to other transformations like scaling/stretching and rotatinig.\n",
    "    - sometimes we *don't* actually want *translation equivariant*... e.g., different convolutions might be different in different parts of an image\n",
    "- *invariance*:    \n",
    "    - *\"Pooling\"* (after the initial convolution and the so called \"director stage\", i.e., activations on the covolution) aggregates \"local\" convolutions in close proximity which results in images being *invariant* to small translations of the inputs\n",
    "        - some features, e.g., eyes, are like this -- *exact* location is not important; however, other targets, e.g., lines forming a corner, can benefit from exact locations \n",
    "    - *max pooling* takes the largest of the activation values, but averaging or an $L^2$ norm of the local (within some rectangle) activation values are other choices\n",
    "    - pooling is of course also computationally advantageous if the local regions are *strided* so as to form a partition pooling providing a local reducer function\n",
    "    - we could also apply pooling over the outputs of different convolutions: this would mean that these convolutions can all measure the same output differently (e.g., when max pooling always just picks the max of them), so the output would then be *invariant to the different convolutions*\n",
    "    \n",
    "- **variable sized inputs**: since convolution is \"swept\" over or across the input the input shapes don't initially matter in that you can still extract features across the input using a kernel; and using a predefined fixed number of pooling partions with implied regions determined by relative percentages that rescale for different means your pooling function will always produce the same sized output \n",
    "\n",
    "### the Interpretation of  CNNs as a Bayesian *prior*\n",
    "- CNNs simply implement useful prior knowledge that surely can help, e.g.:\n",
    "    - translation invariance prior\n",
    "    - feature extraction locality prior \n",
    "    - whereas they do not implement a *permutation invariant* prior (i.e., as fully-connected network) which can theoretically learn topology (the relationship between the different parts of the input) without relying upon natural input structure (e.g., images purmuted in the same manner)\n",
    "\n",
    "\n",
    "\n",
    "## what *convolutions* are\n",
    "\n",
    "- Convolutions are useful for grid-like topology... 1D, 2D, etc....\n",
    "- \"Convolutions\" are mathematical operations that (in discrete, finite contexts) can be computd as specific particular forms of matrix multiplication\n",
    "    - NN \"Convolutions\" actually though tend to (more technically correctly) refer to *cross-correlations*\n",
    "    - The traditional definition of a convolution of the *input function* $f(t)$ with *kernel function* $k(r)$ is the *feature map* $m(t_0) = \\int f(t) k(t_0-t) dt$ where the *kernel function* is evaluated at the displaced value $t_0-t$ relative to the *input function* which is evaluated at $t$\n",
    "        - this effectvely makes $t_0$ \"the origin\" and \"weights\" values $f(t)$ by how far they are \"before the origin\" $t_0-t$ according to the \"(kernal) weigting function\" $k(t_0-t)$\n",
    "        - this means $f(t)$ is integrated against and over every value of $k(r)$ evaluated relative to an origin point $t_0$\n",
    "    - Proper convolutions following this definition are usually written in shorthand as $m(t) = (f*k)(t)$\n",
    "    - In practical applied settings *convolutions* will be evaluated in a discrete manner $m(t_0) = (f*k)(t_0) = \\sum_t f(t) k(t_0-t)$\n",
    "        - Over higher dimensions this looks like this:  $m(t_0,s_0) = (f*k)(t_0,s_0) = \\sum_t \\sum_s f(t,s) k(t_0-t, s_0-s)$\n",
    "- *Cross-correlation* (which is actually what is often meant by \"convoltution\" in NN contexts) is a slight variant of this: $(f*k)(t_0,s_0) = \\sum_t \\sum_s f(t_0+t,s_0+s) k(t, s)$\n",
    "    \n",
    "    - Implementing these *cross-correlations* as matrix multipllications entails usage of a special *Toeplitz matrix* [in which \"each row of the matrix is constrained to be equal to the row above shifted by one element\"] as the \"kernel\" matrix. \n",
    "    - A two-dimensional matrix which implements a discrete convolutional calculation is called a *doubly block circulant matrix*\n",
    "    - Because these sorts of matrices are often very sparse, these matrix multiplications are done in a numerically efficient manner:\n",
    "        - sparse matrix multiplication can avoid all the \"0\" multiplications, and memory referencing values in the sparse matrix can reuse repeated values of the same kernels from the same location in memory as opposed to storing the same values everywhere in the sparse matrix\n",
    "        \n",
    "## some further considerations regarding *convolutions* \n",
    "\n",
    "- gradients of neural networks are implemented in similar, but distinct formula's as the \"convolution\" formula (page 351)\n",
    "- convolutions as outputs with labels on the pixels produce mask and object detection rules models\n",
    "    - choices about output size should be handled intentionally with padding\n",
    "    - A RNN structure can be used to repeatedly refine masking predictions as givein in figure 9.17\n",
    "- Garbor functions are a model for kernels that seem to modol human V1 vision cells       \n",
    "- learning, rather than predefining pooling in network architectures seems potentially interesting\n",
    "    - pooling can present specific challenges within certain network contexts, which will be addressed later    \n",
    "\n",
    "\n",
    "        \n",
    "## how very computationally demanding CNNs are\n",
    "\n",
    "- Computational efficiency can be very important\n",
    "    - Fourier transforms can help speed things up\n",
    "    - *Separable* kernels can be processed univariately and as well are faster that way, when applicable\n",
    "    - instad of the computation needed for fitting these things, sometimes kernels might just be specified randomly (which works surprisingly approximately well, perhaps at least for comparing potential competing architectures), designed by hand, or they can be created in an unsupervised manner\n",
    "        - e.g., K-means on the local data within a kernel region can be used as the kernel matrices at those locations\n",
    "            - this might provide regularization; or, it might just enable larger, richer network architectures\n",
    "    - traininig layers in isolation sequentially (i.e., greedy pretraining, see ch 8) can be another potential strategy to help fit layers, or even small patches can be used to fit the kernel without even using the full model (although these approaches are less popular these days given the increasingly powerful compute available)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
